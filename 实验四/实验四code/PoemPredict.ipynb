{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07825673-02f2-49d6-8d56-14aca34992c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n",
      "Collecting ipywidgets\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/58/6a/9166369a2f092bd286d24e6307de555d63616e8ddb373ebad2b5635ca4cd/ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "\u001b[K     |████████████████████████████████| 139 kB 21.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: traitlets>=4.3.1 in /home/ma-user/anaconda3/envs/PyTorch-1.10.2/lib/python3.7/site-packages (from ipywidgets) (5.5.0)\n",
      "Collecting comm>=0.1.3\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/fe/47/0133ac1b7dc476ed77710715e98077119b3d9bae56b13f6f9055e7da1c53/comm-0.1.4-py3-none-any.whl (6.6 kB)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/ma-user/anaconda3/envs/PyTorch-1.10.2/lib/python3.7/site-packages (from ipywidgets) (7.34.0)\n",
      "Collecting widgetsnbextension~=4.0.14\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/ca/51/5447876806d1088a0f8f71e16542bf350918128d0a69437df26047c8e46f/widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 40.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jupyterlab_widgets~=3.0.15\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/43/6a/ca128561b22b60bd5a0c4ea26649e68c8556b82bc70a0c396eebc977fe86/jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "\u001b[K     |████████████████████████████████| 216 kB 15.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/ma-user/anaconda3/envs/PyTorch-1.10.2/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.31)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/ma-user/anaconda3/envs/PyTorch-1.10.2/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: pickleshare in /home/ma-user/anaconda3/envs/PyTorch-1.10.2/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: matplotlib-inline in /home/ma-user/anaconda3/envs/PyTorch-1.10.2/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: decorator in /home/ma-user/anaconda3/envs/PyTorch-1.10.2/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: pygments in /home/ma-user/anaconda3/envs/PyTorch-1.10.2/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets) (2.13.0)\n",
      "Requirement already satisfied: backcall in /home/ma-user/anaconda3/envs/PyTorch-1.10.2/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/ma-user/anaconda3/envs/PyTorch-1.10.2/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets) (65.5.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/ma-user/anaconda3/envs/PyTorch-1.10.2/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/ma-user/anaconda3/envs/PyTorch-1.10.2/lib/python3.7/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/ma-user/anaconda3/envs/PyTorch-1.10.2/lib/python3.7/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/ma-user/anaconda3/envs/PyTorch-1.10.2/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Installing collected packages: widgetsnbextension, jupyterlab-widgets, comm, ipywidgets\n",
      "Successfully installed comm-0.1.4 ipywidgets-8.1.7 jupyterlab-widgets-3.0.15 widgetsnbextension-4.0.14\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/home/ma-user/anaconda3/envs/PyTorch-1.10.2/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d246b564-0ef7-4587-b2a6-c061854cb965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 1. 导入必要的库\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset # 生成时非必需，但为原始notebook内容\n",
    "import torch.optim as optim # 生成时非必需\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24e99319-66aa-4ef0-ac3b-d44f4e04aff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据和映射关系加载完成。\n",
      "词汇表大小: 8293\n"
     ]
    }
   ],
   "source": [
    "# ## 2. 加载数据和映射关系\n",
    "# 确保 tang.npz 文件在同一目录下，或提供正确的路径\n",
    "try:\n",
    "    tang_file = np.load(\"tang.npz\", allow_pickle=True)\n",
    "    word2ix = tang_file['word2ix'].item()\n",
    "    idx2word = tang_file['ix2word'].item()\n",
    "    data = tang_file['data'] # 为上下文加载，在下面的生成函数中不直接使用\n",
    "    print(\"数据和映射关系加载完成。\")\n",
    "    print(f\"词汇表大小: {len(word2ix)}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"错误: tang.npz 文件未找到。请确保它在正确的路径下。\")\n",
    "    # 使用虚拟值初始化，以允许脚本进一步运行，但生成会失败。\n",
    "    word2ix = {\"<START>\": 0, \"<EOP>\": 1, \"，\": 2, \"。\": 3, \"山\":4, \"水\":5, \"重\":6, \"庆\":7, \"大\":8, \"好\":9, \"河\":10, \"<unk>\": 11}\n",
    "    idx2word = {v: k for k, v in word2ix.items()}\n",
    "    data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60a6d08f-299b-4588-acf7-1eada16f453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 3. 定义模型 (`PoemNet`)\n",
    "# 这应该与训练时使用的定义相同。\n",
    "class PoemNet(nn.Module):\n",
    "    def __init__(self, vocab_size_param, embedding_dim_param, hidden_dim_param):\n",
    "        super(PoemNet, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size_param, embedding_dim_param)\n",
    "        self.lstm = nn.LSTM(embedding_dim_param, hidden_dim_param, num_layers=2, batch_first=True, dropout=0.5)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim_param, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(2048, 4096),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, vocab_size_param)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_seq, hidden=None):\n",
    "        embedded = self.embeddings(input_seq)\n",
    "        lstm_out, hidden = self.lstm(embedded, hidden)\n",
    "        output = self.fc(lstm_out)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab4d93c7-e5f8-4554-985b-1e8a18b77cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用的设备: cuda:0\n",
      "尝试加载完整模型 model.h5...\n",
      "模型 model.h5 加载成功。\n"
     ]
    }
   ],
   "source": [
    "# ## 4. 加载已训练模型\n",
    "# Determine the device to use (CUDA GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用的设备: {device}\") # Display the device being used\n",
    "\n",
    "VOCAB_SIZE = len(word2ix) \n",
    "EMBEDDING_DIM = 200\n",
    "HIDDEN_DIM = 1024\n",
    "\n",
    "my_net = None # Initialize my_net to None\n",
    "\n",
    "try:\n",
    "    # User guarantees that \"model.h5\" is a complete model file, so load it directly.\n",
    "    print(\"尝试加载完整模型 model.h5...\")\n",
    "    # Load the entire model from the specified file.\n",
    "    # map_location=device ensures the model is loaded onto the correct device from the start.\n",
    "    my_net = torch.load(\"model.h5\", map_location=device)\n",
    "\n",
    "    my_net = my_net.to(device)\n",
    "    \n",
    "    if my_net: # Check if the model was successfully loaded and assigned to my_net\n",
    "        my_net.eval() # Set the model to evaluation mode. This disables layers like Dropout.\n",
    "        print(\"模型 model.h5 加载成功。\")\n",
    "    else:\n",
    "        # This case should ideally be caught by an exception if torch.load fails (e.g., returns None or a non-model object).\n",
    "        # Kept as an additional safeguard.\n",
    "        print(\"错误：模型未能正确加载。torch.load 可能返回了 None 或非预期对象。\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    # Handle the case where the model file does not exist at the specified path.\n",
    "    print(\"错误: model.h5 文件未找到。请确保它在正确的路径下。\")\n",
    "    my_net = None # Ensure my_net remains None if loading fails\n",
    "except Exception as e:\n",
    "    # Catch other potential errors during model loading, such as a corrupted file or an invalid PyTorch model file.\n",
    "    print(f\"加载模型 model.h5 时发生一般性错误: {e}\")\n",
    "    print(\"请确保 model.h5 是一个有效的 PyTorch 完整模型文件。\")\n",
    "    my_net = None # Ensure my_net remains None if loading fails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "171dcb8b-6a13-4962-87cb-9610bf7f772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 5. generate_poem 方法 (根据 docx 结构补全)\n",
    "def generate_poem(my_words, num_chars_per_line): # 移除了 N_GRAM_SIZE 参数\n",
    "    '''\n",
    "    根据前文my_words生成一首固定四句话的诗。\n",
    "    my_words: 提示词字符串。\n",
    "    num_chars_per_line: 每行诗的字数 (5或7)。\n",
    "    N_GRAM_SIZE 将硬编码为 2 (二元组检查)。\n",
    "    '''\n",
    "    N_GRAM_SIZE = 2 # 硬编码 N_GRAM_SIZE 为 2\n",
    "\n",
    "    if my_net is None or not word2ix or not idx2word:\n",
    "        return \"错误：模型或数据映射未正确加载。\"\n",
    "    if num_chars_per_line not in [5, 7]:\n",
    "        return \"错误：每行字数必须是5或7。\"\n",
    "\n",
    "    def is_chinese_char(char_unicode): # 判断是否为中文字符的辅助函数\n",
    "        if char_unicode is None: return False\n",
    "        if '\\u4e00' <= char_unicode <= '\\u9fff':\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "    def __generate_next(idx, hidden=None): \n",
    "        \"\"\"\n",
    "        根据输入索引和隐藏状态输出下一个预测词的logits和新的隐藏状态。\n",
    "        \"\"\"\n",
    "        input_tensor = torch.tensor([[idx]], dtype=torch.long).to(device)\n",
    "        output_logits, new_hidden = my_net(input_tensor, hidden)\n",
    "        return output_logits, new_hidden\n",
    "\n",
    "    start_token_idx = word2ix.get(\"<START>\") \n",
    "    if start_token_idx is None: return \"错误: <START> 不在词典中。\"\n",
    "    \n",
    "    current_logits, hidden = __generate_next(start_token_idx) \n",
    "\n",
    "    poem_lines = [] \n",
    "    current_line_chars = [] # 存储当前正在构建的诗句中的汉字\n",
    "    \n",
    "    # 用于n-gram检查的集合，存储当前行已经出现过的n-gram字符串\n",
    "    current_line_ngrams = set() \n",
    "\n",
    "    unk_token_idx = word2ix.get('<unk>') \n",
    "    \n",
    "    default_fallback_char_str = \"之\" \n",
    "    fallback_char_idx = word2ix.get(default_fallback_char_str)\n",
    "    actual_fallback_char_for_display = default_fallback_char_str\n",
    "\n",
    "    if fallback_char_idx is None: \n",
    "        if unk_token_idx is not None:\n",
    "            fallback_char_idx = unk_token_idx\n",
    "            actual_fallback_char_for_display = idx2word.get(unk_token_idx, \"<unk>\")\n",
    "            print(f\"警告: 回退字符 '之' 不在词典中，将使用 '{actual_fallback_char_for_display}'。\")\n",
    "        else: \n",
    "            found_alternative_fallback = False\n",
    "            for idx_val_iter, char_val_iter in idx2word.items():\n",
    "                if is_chinese_char(char_val_iter):\n",
    "                    fallback_char_idx = idx_val_iter\n",
    "                    actual_fallback_char_for_display = char_val_iter\n",
    "                    print(f\"警告: 回退字符 '之' 和 '<unk>' 均不在词典中，将使用词典中的 '{actual_fallback_char_for_display}'。\")\n",
    "                    found_alternative_fallback = True\n",
    "                    break\n",
    "            if not found_alternative_fallback:\n",
    "                 return \"错误：词汇表中没有可用的中文回退字符 (例如 '之' 或 '<unk>')。\"\n",
    "\n",
    "    # 1. 处理提示词 (my_words)\n",
    "    processed_prompt_chars_for_line = [] \n",
    "    temp_current_logits_after_prompt = current_logits \n",
    "    temp_hidden_after_prompt = hidden \n",
    "\n",
    "    if my_words: \n",
    "        for char_in_prompt in my_words:\n",
    "            if len(poem_lines) == 4: \n",
    "                break\n",
    "            \n",
    "            char_idx_for_feed = word2ix.get(char_in_prompt)\n",
    "            if char_idx_for_feed is None:\n",
    "                if unk_token_idx is not None:\n",
    "                    print(f\"警告：提示词中的字 '{char_in_prompt}' 不在词典中，将使用'<unk>'影响隐藏状态。\")\n",
    "                    char_idx_for_feed = unk_token_idx\n",
    "                else: \n",
    "                    print(f\"警告：提示词中的字 '{char_in_prompt}' 未知且无'<unk>'，跳过此字对隐藏状态的影响。\")\n",
    "                    continue \n",
    "\n",
    "            temp_current_logits_after_prompt, temp_hidden_after_prompt = __generate_next(char_idx_for_feed, temp_hidden_after_prompt)\n",
    "\n",
    "            if is_chinese_char(char_in_prompt):\n",
    "                processed_prompt_chars_for_line.append(char_in_prompt)\n",
    "                # 更新提示词部分的n-gram (如果N_GRAM_SIZE > 1)\n",
    "                if N_GRAM_SIZE > 1 and len(processed_prompt_chars_for_line) >= N_GRAM_SIZE:\n",
    "                    current_line_ngrams.add(\"\".join(processed_prompt_chars_for_line[-(N_GRAM_SIZE):]))\n",
    "\n",
    "                if len(processed_prompt_chars_for_line) == num_chars_per_line:\n",
    "                    poem_lines.append(\"\".join(processed_prompt_chars_for_line))\n",
    "                    processed_prompt_chars_for_line = []\n",
    "                    current_line_ngrams.clear() # 新的一行，清空n-gram记录\n",
    "        \n",
    "        current_logits = temp_current_logits_after_prompt\n",
    "        hidden = temp_hidden_after_prompt\n",
    "        \n",
    "        current_line_chars.extend(processed_prompt_chars_for_line)\n",
    "\n",
    "\n",
    "    # 2. 生成诗歌的其余部分 (目标是总共4行)\n",
    "    while len(poem_lines) < 4:\n",
    "        while len(current_line_chars) < num_chars_per_line:\n",
    "            output_logits_squeezed = current_logits.squeeze()\n",
    "            top_k_probs, top_k_indices = torch.topk(output_logits_squeezed, k=10) \n",
    "            \n",
    "            chosen_char_for_line = None\n",
    "            chosen_idx_for_next_step = -1\n",
    "            \n",
    "            # 尝试从top_k中选择一个不构成重复n-gram的字符\n",
    "            for i_k in range(top_k_indices.size(0)):\n",
    "                potential_idx = top_k_indices[i_k].item()\n",
    "                potential_char = idx2word.get(potential_idx)\n",
    "\n",
    "                if potential_char and potential_char != \"<EOP>\" and is_chinese_char(potential_char):\n",
    "                    is_ngram_repeated = False\n",
    "                    if N_GRAM_SIZE > 1 and len(current_line_chars) >= N_GRAM_SIZE - 1:\n",
    "                        # 构建测试的n-gram\n",
    "                        # 前 N_GRAM_SIZE - 1 个字符来自 current_line_chars 的末尾\n",
    "                        # 最后一个字符是 potential_char\n",
    "                        prefix_for_ngram = \"\".join(current_line_chars[-(N_GRAM_SIZE - 1):])\n",
    "                        test_ngram = prefix_for_ngram + potential_char\n",
    "                        if test_ngram in current_line_ngrams:\n",
    "                            is_ngram_repeated = True\n",
    "                    \n",
    "                    if not is_ngram_repeated:\n",
    "                        chosen_char_for_line = potential_char\n",
    "                        chosen_idx_for_next_step = potential_idx\n",
    "                        break # 找到了合适的非重复字符\n",
    "            \n",
    "            if chosen_char_for_line is None: # 如果所有top_k候选都导致重复或不合适\n",
    "                # 退回到选择第一个有效的中文字符 (即使它可能导致重复)\n",
    "                # 或者使用全局回退字符\n",
    "                # 为了保证生成，我们这里选择第一个有效中文字符，或者全局回退\n",
    "                first_valid_candidate_char = None\n",
    "                first_valid_candidate_idx = -1\n",
    "                for i_k_fallback in range(top_k_indices.size(0)):\n",
    "                    potential_idx_fb = top_k_indices[i_k_fallback].item()\n",
    "                    potential_char_fb = idx2word.get(potential_idx_fb)\n",
    "                    if potential_char_fb and potential_char_fb != \"<EOP>\" and is_chinese_char(potential_char_fb):\n",
    "                        first_valid_candidate_char = potential_char_fb\n",
    "                        first_valid_candidate_idx = potential_idx_fb\n",
    "                        break\n",
    "                \n",
    "                if first_valid_candidate_char:\n",
    "                    chosen_char_for_line = first_valid_candidate_char\n",
    "                    chosen_idx_for_next_step = first_valid_candidate_idx\n",
    "                    # print(f\"提示: N-gram检查后无优选，选择top-k首个有效字符'{chosen_char_for_line}'\")\n",
    "                else: # 连top-k里都没有有效中文字符了\n",
    "                    chosen_char_for_line = actual_fallback_char_for_display\n",
    "                    chosen_idx_for_next_step = fallback_char_idx\n",
    "                    # print(f\"提示: N-gram检查后无优选且top-k无效，使用全局回退字符 '{chosen_char_for_line}'\")\n",
    "\n",
    "\n",
    "            current_line_chars.append(chosen_char_for_line)\n",
    "            # 更新当前行的n-gram记录\n",
    "            if N_GRAM_SIZE > 1 and len(current_line_chars) >= N_GRAM_SIZE:\n",
    "                current_line_ngrams.add(\"\".join(current_line_chars[-(N_GRAM_SIZE):]))\n",
    "\n",
    "            current_logits, hidden = __generate_next(chosen_idx_for_next_step, hidden)\n",
    "        \n",
    "        poem_lines.append(\"\".join(current_line_chars))\n",
    "        current_line_chars = [] \n",
    "        current_line_ngrams.clear() # 新的一行，清空n-gram记录\n",
    "        \n",
    "    # 3. 格式化输出\n",
    "    if not poem_lines: \n",
    "        return \"（未生成诗歌）\"\n",
    "\n",
    "    formatted_poem_str = \"\" \n",
    "    for i, line_content in enumerate(poem_lines): \n",
    "        formatted_poem_str += line_content \n",
    "        if i == len(poem_lines) - 1: \n",
    "            formatted_poem_str += \"。\" \n",
    "        else: \n",
    "            formatted_poem_str += \"，\\n\" \n",
    "            \n",
    "    return formatted_poem_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56d86329-5e79-4526-87f1-4f6debc31941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 测试 generate_poem (五言) ---\n",
      "generate_poem(\"重庆\", 5) 输出:\n",
      "重庆三千里，\n",
      "上游仙客处，\n",
      "寻常到处寻，\n",
      "常久处栽花。\n",
      "\n",
      "\n",
      "--- 测试 generate_poem (五言) ---\n",
      "generate_poem(\"无提示词\", 5) 输出:\n",
      "一片寒泉一，\n",
      "片云鬟月映，\n",
      "明月中天月，\n",
      "明月光明灭。\n",
      "\n",
      "\n",
      "--- 测试 generate_poem (五言) ---\n",
      "generate_poem(\"春眠不觉晓\", 5) 输出:\n",
      "春眠不觉晓，\n",
      "光迟日迟迟，\n",
      "迟迟日迟回，\n",
      "日迟迟日暮。\n",
      "\n",
      "\n",
      "--- 测试 generate_poem (七言) ---\n",
      "generate_poem(\"青山隐隐\", 7) 输出:\n",
      "青山隐隐沦天地，\n",
      "宽深处处寻常在，\n",
      "处寻常处处无人，\n",
      "到处无人到世间。\n",
      "\n",
      "\n",
      "--- 测试 generate_poem (七言) ---\n",
      "generate_poem(\"我爱重庆大学\", 7) 输出:\n",
      "我爱重庆大学名，\n",
      "张李李杜公子诗，\n",
      "人不可论高臥玉，\n",
      "堂中夜月明月光。\n",
      "\n",
      "\n",
      "--- 测试 generate_poem (七言) ---\n",
      "generate_poem(\"自然语言处理\", 7) 输出:\n",
      "自然语言处理衰，\n",
      "荣久不知衰朽事，\n",
      "牵愁处处无人识，\n",
      "世情难尽处深处。\n",
      "\n",
      "\n",
      "--- 测试 generate_poem (提示词超出一行，五言) ---\n",
      "generate_poem(\"白日依山尽黄河\", 5) 输出:\n",
      "白日依山尽，\n",
      "黄河水水流，\n",
      "无穷处处无，\n",
      "人到处寻常。\n",
      "\n",
      "\n",
      "--- 测试 generate_poem (提示词超出一行，七言) ---\n",
      "generate_poem(\"故人西辞黄鹤楼烟花\", 7) 输出:\n",
      "故人西辞黄鹤楼，\n",
      "烟花之东流水上，\n",
      "天涯去不回头去，\n",
      "不回头白头时时。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 测试 generate_poem\n",
    "if my_net:\n",
    "    print(\"\\n--- 测试 generate_poem (五言) ---\")\n",
    "    generated_text_5_ngram = generate_poem(\"重庆\", num_chars_per_line=5)\n",
    "    print(f\"generate_poem(\\\"重庆\\\", 5) 输出:\\n{generated_text_5_ngram}\\n\")\n",
    "    \n",
    "    print(\"\\n--- 测试 generate_poem (五言) ---\")\n",
    "    generated_text_5_ngram = generate_poem(\"\", 5)\n",
    "    print(f\"generate_poem(\\\"无提示词\\\", 5) 输出:\\n{generated_text_5_ngram}\\n\")\n",
    "    \n",
    "    print(\"\\n--- 测试 generate_poem (五言) ---\")\n",
    "    generated_text_full_prompt_5_ngram = generate_poem(\"春眠不觉晓\", 5)\n",
    "    print(f\"generate_poem(\\\"春眠不觉晓\\\", 5) 输出:\\n{generated_text_full_prompt_5_ngram}\\n\")\n",
    "\n",
    "    print(\"\\n--- 测试 generate_poem (七言) ---\")\n",
    "    generated_text_7_ngram = generate_poem(\"青山隐隐\", num_chars_per_line=7)\n",
    "    print(f\"generate_poem(\\\"青山隐隐\\\", 7) 输出:\\n{generated_text_7_ngram}\\n\")\n",
    "\n",
    "    print(\"\\n--- 测试 generate_poem (七言) ---\")\n",
    "    generated_text_7_ngram = generate_poem(\"我爱重庆大学\", 7)\n",
    "    print(f\"generate_poem(\\\"我爱重庆大学\\\", 7) 输出:\\n{generated_text_7_ngram}\\n\")\n",
    "    \n",
    "    print(\"\\n--- 测试 generate_poem (七言) ---\")\n",
    "    generated_text_7_ngram = generate_poem(\"自然语言处理\", 7)\n",
    "    print(f\"generate_poem(\\\"自然语言处理\\\", 7) 输出:\\n{generated_text_7_ngram}\\n\")\n",
    "\n",
    "    print(\"\\n--- 测试 generate_poem (提示词超出一行，五言) ---\") \n",
    "    generated_text_overflow_prompt_5_ngram3 = generate_poem(\"白日依山尽黄河\", 5)\n",
    "    print(f\"generate_poem(\\\"白日依山尽黄河\\\", 5) 输出:\\n{generated_text_overflow_prompt_5_ngram3}\\n\")\n",
    "    \n",
    "    print(\"\\n--- 测试 generate_poem (提示词超出一行，七言) ---\") \n",
    "    generated_text_overflow_prompt_7_ngram3 = generate_poem(\"故人西辞黄鹤楼烟花\", 7)\n",
    "    print(f\"generate_poem(\\\"故人西辞黄鹤楼烟花\\\", 7) 输出:\\n{generated_text_overflow_prompt_7_ngram3}\\n\")\n",
    "\n",
    "else:\n",
    "    print(\"generate_poem: 模型未加载，无法测试。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a457a52d-2192-4530-accb-32c9a3c04342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 6. acrostic_poetry 方法 (根据 docx 结构补全)\n",
    "# acrostic_poetry 的 n-gram 优化逻辑与 generate_poem 类似。\n",
    "def acrostic_poetry(my_words, num_chars_per_line): \n",
    "    if my_net is None or not word2ix or not idx2word:\n",
    "        print(\"错误：模型或数据映射未正确加载。\")\n",
    "        return\n",
    "    if num_chars_per_line not in [5, 7]:\n",
    "        print(\"错误：每行字数必须是5或7。\")\n",
    "        return\n",
    "\n",
    "    def is_chinese_char(char): # 内部辅助函数：判断是否为中文字符\n",
    "        if char is None: return False\n",
    "        return '\\u4e00' <= char <= '\\u9fff'\n",
    "\n",
    "    def __generate_next_acrostic(idx, hidden=None): # 重命名以避免在脚本环境中与其他__generate_next冲突\n",
    "        input_tensor = torch.tensor([[idx]], dtype=torch.long).to(device)\n",
    "        output_logits, new_hidden = my_net(input_tensor, hidden)\n",
    "        return output_logits, new_hidden\n",
    "\n",
    "    def __generate_line_acrostic(word_head, hidden_state, current_num_chars_per_line): # 生成藏头诗的一行\n",
    "        sentence_chars_for_line = [word_head] # 诗行以藏头字开始\n",
    "        current_line_ngrams_ac = set() # 用于当前藏头诗行的n-gram记录\n",
    "        \n",
    "        unk_token_idx_ac = word2ix.get(\"<unk>\") # 未知标记索引\n",
    "        current_char_idx_ac = word2ix.get(word_head) # 藏头字索引\n",
    "\n",
    "        if current_char_idx_ac is None: \n",
    "            print(f\"警告：藏头字 '{word_head}' 不在词典中。将使用 '<unk>' (如果存在) 更新隐藏状态。\")\n",
    "            if unk_token_idx_ac is not None:\n",
    "                current_char_idx_ac = unk_token_idx_ac\n",
    "            else:\n",
    "                print(f\"错误：藏头字 '{word_head}' 未知且词典中无'<unk>'。此行可能不准确或不完整。\")\n",
    "                default_fallback_char_str_ac = \"之\"\n",
    "                fallback_char_idx_ac = word2ix.get(default_fallback_char_str_ac, unk_token_idx_ac) \n",
    "                if fallback_char_idx_ac is None: \n",
    "                    found_fallback_ac = False\n",
    "                    for idx_val_ac, char_val_iter_ac in idx2word.items():\n",
    "                        if is_chinese_char(char_val_iter_ac):\n",
    "                            fallback_char_idx_ac = idx_val_ac\n",
    "                            found_fallback_ac = True\n",
    "                            break\n",
    "                    if not found_fallback_ac:\n",
    "                        print(\"错误：无法为藏头诗内容生成找到回退字符。\")\n",
    "                        return sentence_chars_for_line, hidden_state \n",
    "                current_char_idx_ac = fallback_char_idx_ac \n",
    "\n",
    "        temp_logits, temp_hidden = __generate_next_acrostic(current_char_idx_ac, hidden_state) \n",
    "        \n",
    "        num_additional_chars_to_generate = current_num_chars_per_line - 1 \n",
    "        \n",
    "        default_fallback_content_char_str = \"之\"\n",
    "        actual_fallback_content_char_display = default_fallback_content_char_str\n",
    "        fallback_content_char_idx = word2ix.get(default_fallback_content_char_str)\n",
    "\n",
    "        if fallback_content_char_idx is None:\n",
    "            if unk_token_idx_ac is not None:\n",
    "                fallback_content_char_idx = unk_token_idx_ac\n",
    "                actual_fallback_content_char_display = idx2word.get(unk_token_idx_ac, \"<unk>\")\n",
    "            else: \n",
    "                found_alt_fallback_ac = False\n",
    "                for idx_val_iter_ac, char_val_iter_ac in idx2word.items():\n",
    "                    if is_chinese_char(char_val_iter_ac):\n",
    "                        fallback_content_char_idx = idx_val_iter_ac\n",
    "                        actual_fallback_content_char_display = char_val_iter_ac\n",
    "                        found_alt_fallback_ac = True\n",
    "                        break\n",
    "                if not found_alt_fallback_ac:\n",
    "                    print(\"错误: 藏头诗内容生成部分无法找到回退字符!\")\n",
    "                    return sentence_chars_for_line, temp_hidden \n",
    "\n",
    "        current_logits_for_loop = temp_logits \n",
    "        N_GRAM_SIZE_ACROSTIC = 2 # 为藏头诗也定义一个n-gram大小，可以设为参数\n",
    "\n",
    "        for _ in range(num_additional_chars_to_generate): \n",
    "            top_k_probs_ac, top_k_indices_ac = torch.topk(current_logits_for_loop.squeeze(), k=5) \n",
    "            selected_char_candidate = None \n",
    "            selected_idx_candidate = -1 \n",
    "\n",
    "            for i_k_ac in range(top_k_indices_ac.size(0)):\n",
    "                potential_idx_ac = top_k_indices_ac[i_k_ac].item()\n",
    "                potential_char_ac = idx2word.get(potential_idx_ac)\n",
    "                if potential_char_ac and is_chinese_char(potential_char_ac):\n",
    "                    is_ngram_repeated_ac = False\n",
    "                    if N_GRAM_SIZE_ACROSTIC > 1 and len(sentence_chars_for_line) >= N_GRAM_SIZE_ACROSTIC -1 : \n",
    "                        prefix_ac = \"\".join(sentence_chars_for_line[-(N_GRAM_SIZE_ACROSTIC -1):])\n",
    "                        test_ngram_ac = prefix_ac + potential_char_ac\n",
    "                        if test_ngram_ac in current_line_ngrams_ac:\n",
    "                            is_ngram_repeated_ac = True\n",
    "                    \n",
    "                    if not is_ngram_repeated_ac:\n",
    "                        selected_char_candidate = potential_char_ac\n",
    "                        selected_idx_candidate = potential_idx_ac\n",
    "                        break\n",
    "            \n",
    "            char_to_append_to_line_ac: str \n",
    "            idx_for_next_feed_ac: int\n",
    "\n",
    "            if selected_char_candidate: \n",
    "                char_to_append_to_line_ac = selected_char_candidate\n",
    "                idx_for_next_feed_ac = selected_idx_candidate\n",
    "            else: \n",
    "                char_to_append_to_line_ac = actual_fallback_content_char_display\n",
    "                idx_for_next_feed_ac = fallback_content_char_idx\n",
    "            \n",
    "            sentence_chars_for_line.append(char_to_append_to_line_ac)\n",
    "            if N_GRAM_SIZE_ACROSTIC > 1 and len(sentence_chars_for_line) >= N_GRAM_SIZE_ACROSTIC:\n",
    "                current_line_ngrams_ac.add(\"\".join(sentence_chars_for_line[-N_GRAM_SIZE_ACROSTIC:]))\n",
    "            \n",
    "            if _ < num_additional_chars_to_generate -1: \n",
    "                 current_logits_for_loop, temp_hidden = __generate_next_acrostic(idx_for_next_feed_ac, temp_hidden)\n",
    "\n",
    "        return sentence_chars_for_line, temp_hidden \n",
    "\n",
    "\n",
    "    start_token_idx_ac = word2ix.get(\"<START>\") \n",
    "    if start_token_idx_ac is None:\n",
    "        print(\"错误: <START> 不在词典中 (acrostic_poetry)。\")\n",
    "        return\n",
    "\n",
    "    _, overall_hidden_state = __generate_next_acrostic(start_token_idx_ac)\n",
    "    \n",
    "    result_lines_strings = [] \n",
    "    head_char_list = list(my_words) \n",
    "\n",
    "    for head_char_for_line_iter in head_char_list: \n",
    "      line_char_list_ac, overall_hidden_state = __generate_line_acrostic(\n",
    "          head_char_for_line_iter, \n",
    "          overall_hidden_state, \n",
    "          num_chars_per_line\n",
    "      )\n",
    "      result_lines_strings.append(\"\".join(line_char_list_ac)) \n",
    "    \n",
    "    output_string_parts = []\n",
    "    for i, line_text_ac in enumerate(result_lines_strings): \n",
    "        if i == len(result_lines_strings) - 1: \n",
    "            output_string_parts.append(line_text_ac + \"。\") \n",
    "        else: \n",
    "            output_string_parts.append(line_text_ac + \"，\") \n",
    "\n",
    "    print(\"\\n\".join(output_string_parts)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c7aa41a-8c43-4021-95cd-c7dff76909a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 测试 acrostic_poetry (五言) ---\n",
      "大道无心处，\n",
      "好不知之心，\n",
      "河水之中央，\n",
      "山之水堋堋。\n",
      "\n",
      "--- 测试 acrostic_poetry (五言) ---\n",
      "我家山水上，\n",
      "想无穷处处，\n",
      "吃然不知何，\n",
      "面上天涯去，\n",
      "条似雪中人。\n",
      "\n",
      "--- 测试 acrostic_poetry (七言) ---\n",
      "春风吹柳絮如丝，\n",
      "花处处春深绿野，\n",
      "十年时处处寻春，\n",
      "里处处寻常少处。\n",
      "\n",
      "--- 测试 acrostic_poetry (七言) ---\n",
      "好鸟不知归去去，\n",
      "想无人到处寻常，\n",
      "睡不得行人说得，\n",
      "大师人不识人间，\n",
      "觉处处寻常说处。\n"
     ]
    }
   ],
   "source": [
    "# 测试 acrostic_poetry\n",
    "if my_net:\n",
    "    print(\"\\n--- 测试 acrostic_poetry (五言) ---\")\n",
    "    acrostic_poetry(\"大好河山\", 5) \n",
    "\n",
    "    print(\"\\n--- 测试 acrostic_poetry (五言) ---\")\n",
    "    acrostic_poetry(\"我想吃面条\", 5) \n",
    "    \n",
    "    print(\"\\n--- 测试 acrostic_poetry (七言) ---\")\n",
    "    acrostic_poetry(\"春花十里\", 7) \n",
    "    \n",
    "    print(\"\\n--- 测试 acrostic_poetry (七言) ---\")\n",
    "    acrostic_poetry(\"好想睡大觉\", 7) \n",
    "else:\n",
    "    print(\"acrostic_poetry: 模型未加载，无法测试。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-1.10.2",
   "language": "python",
   "name": "pytorch-1.10.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
