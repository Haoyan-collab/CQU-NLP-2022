{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad460b4f-229c-4b04-901f-72a6c58baec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n",
      "Requirement already satisfied: pip in /home/ma-user/anaconda3/envs/TensorFlow-2.1/lib/python3.7/site-packages (21.0.1)\n",
      "Collecting pip\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/8a/6a/19e9fe04fca059ccf770861c7d5721ab4c2aebc539889e97c7977528a53b/pip-24.0-py3-none-any.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 102.6 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.0.1\n",
      "    Uninstalling pip-21.0.1:\n",
      "      Successfully uninstalled pip-21.0.1\n",
      "Successfully installed pip-24.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    " pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9d61aff-3cdd-48e5-81bf-ac80ab189e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n",
      "Collecting jieba\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/c6/cb/18eeb235f833b726522d7ebed54f2278ce28ba9438e3135ab0278d9792a2/jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m114.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: jieba\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=9a814f157dc855e00219fc1353486f6227038543e89fe5e35b25a5a3a9dc73c6\n",
      "  Stored in directory: /home/ma-user/.cache/pip/wheels/86/bc/97/67c05f24b07573fd425039f944f8bc57c8dbc6f2c0bcc046fa\n",
      "Successfully built jieba\n",
      "\u001b[33mDEPRECATION: moxing-framework 2.1.0.5d9c87c8 has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of moxing-framework or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: jieba\n",
      "Successfully installed jieba-0.42.1\n"
     ]
    }
   ],
   "source": [
    "! pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50fcc91f-35a5-4942-a4ab-6a83f6432d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 10:36:37.696781: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6\n",
      "2025-04-28 10:36:37.698227: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6\n",
      "/home/ma-user/anaconda3/envs/TensorFlow-2.1/lib/python3.7/site-packages/requests/__init__.py:104: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "train_data = pd.read_csv('data/train.tsv', sep='\\t')\n",
    "valid_data = pd.read_csv('data/dev.tsv', sep='\\t')\n",
    "test_data = pd.read_csv('data/test.tsv', sep='\\t') \n",
    "x_train, y_train = train_data.text_a.values, train_data.label.values # 训练集\n",
    "x_valid, y_valid = valid_data.text_a.values, valid_data.label.values # 验证集\n",
    "x_test, y_test = test_data.text_a.values, test_data.label.values # 测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a84e4967-44b2-4bf3-85e6-4bccadd46eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_a</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.4寸笔记本的键盘确实爽，基本跟台式机差不多了，蛮喜欢数字小键盘，输数字特方便，样子也很...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>房间太小。其他的都一般。。。。。。。。。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.接电源没有几分钟,电源适配器热的不行. 2.摄像头用不起来. 3.机盖的钢琴漆，手不能摸...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>今天才知道这书还有第6卷,真有点郁闷:为什么同一套书有两种版本呢?当当网是不是该跟出版社商量...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9141</th>\n",
       "      <td>看过该书，感觉中医暂时不会消亡，尚有一、二十株老树活着，还有毛以林、黄煌、刘力红等一批有一定...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9142</th>\n",
       "      <td>这本书没读到底，不是特别喜欢。完全可以用序中的评价来表达我的感受：可以包容，却不想实践。除了...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9143</th>\n",
       "      <td>虽是观景房,不过我住的楼层太低(19楼)看不到江景,但地点很好,离轻轨临江门站和较场口站(起...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9144</th>\n",
       "      <td>性价比不错，交通方便。行政楼层感觉很好，只是早上8点楼上装修，好吵。 中餐厅档次太低，虽然便...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9145</th>\n",
       "      <td>跟心灵鸡汤没什么本质区别嘛，至少我不喜欢这样读经典，把经典都解读成这样有点去中国化的味道了</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9146 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text_a  label\n",
       "0     选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全...      1\n",
       "1     15.4寸笔记本的键盘确实爽，基本跟台式机差不多了，蛮喜欢数字小键盘，输数字特方便，样子也很...      1\n",
       "2                                  房间太小。其他的都一般。。。。。。。。。      0\n",
       "3     1.接电源没有几分钟,电源适配器热的不行. 2.摄像头用不起来. 3.机盖的钢琴漆，手不能摸...      0\n",
       "4     今天才知道这书还有第6卷,真有点郁闷:为什么同一套书有两种版本呢?当当网是不是该跟出版社商量...      1\n",
       "...                                                 ...    ...\n",
       "9141  看过该书，感觉中医暂时不会消亡，尚有一、二十株老树活着，还有毛以林、黄煌、刘力红等一批有一定...      1\n",
       "9142  这本书没读到底，不是特别喜欢。完全可以用序中的评价来表达我的感受：可以包容，却不想实践。除了...      0\n",
       "9143  虽是观景房,不过我住的楼层太低(19楼)看不到江景,但地点很好,离轻轨临江门站和较场口站(起...      1\n",
       "9144  性价比不错，交通方便。行政楼层感觉很好，只是早上8点楼上装修，好吵。 中餐厅档次太低，虽然便...      1\n",
       "9145      跟心灵鸡汤没什么本质区别嘛，至少我不喜欢这样读经典，把经典都解读成这样有点去中国化的味道了      0\n",
       "\n",
       "[9146 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddaa244f-fad2-46e4-813b-f317a227d238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般',\n",
       "        '15.4寸笔记本的键盘确实爽，基本跟台式机差不多了，蛮喜欢数字小键盘，输数字特方便，样子也很美观，做工也相当不错',\n",
       "        '房间太小。其他的都一般。。。。。。。。。', ...,\n",
       "        '虽是观景房,不过我住的楼层太低(19楼)看不到江景,但地点很好,离轻轨临江门站和较场口站(起点)很近,解放碑就在附近(大约100多公尺吧)!',\n",
       "        '性价比不错，交通方便。行政楼层感觉很好，只是早上8点楼上装修，好吵。 中餐厅档次太低，虽然便宜，但是和酒店档次不相配。',\n",
       "        '跟心灵鸡汤没什么本质区别嘛，至少我不喜欢这样读经典，把经典都解读成这样有点去中国化的味道了'], dtype=object),\n",
       " array([1, 1, 0, ..., 1, 1, 0]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b50c823f-1de2-4cb9-ac4e-b6846f866f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.661 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "vocab = set()\n",
    "cut_docs = train_data.text_a.apply(lambda x: jieba.cut(x)).values\n",
    "for doc in cut_docs:\n",
    "    for word in doc:\n",
    "        if word.strip():\n",
    "            vocab.add(word.strip())\n",
    "\n",
    "# 将词表写入本地vocab.txt文件\n",
    "with open('data/vocab.txt', 'w') as file:\n",
    "    for word in  vocab:\n",
    "        file.write(word)\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e13bbd8-48b0-428a-ab6b-321de0fde01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    embedding_dim = 300 # 词向量维度\n",
    "    max_seq_len = 200 # 文章最大词数\n",
    "    vocab_file = 'data/vocab.txt' # 词汇表文件路径\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6ad0934-49b6-4d1f-b4ef-a8b355fde94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor():\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        # 初始化词和id的映射词典，预留0给padding字符，1给词表中未见过的词\n",
    "        token2idx = {\"[PAD]\": 0, \"[UNK]\": 1} # {word：id}\n",
    "        with open(config.vocab_file, 'r') as reader:\n",
    "            for index, line in enumerate(reader):\n",
    "                token = line.strip()\n",
    "                token2idx[token] = index+2\n",
    "                \n",
    "        self.token2idx = token2idx\n",
    "        \n",
    "    def transform(self, text_list):\n",
    "        # 文本分词，并将词转换成相应的id, 最后不同长度的文本padding长统一长度，后面补0\n",
    "        idx_list = [[self.token2idx.get(word.strip(), self.token2idx['[UNK]']) for word in jieba.cut(text)] for text in text_list]\n",
    "        idx_padding = pad_sequences(idx_list, self.config.max_seq_len, padding='post')\n",
    "        \n",
    "        return idx_padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "696bc2df-ca4d-47da-aafb-49c7b9d6cebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25914,  5479,  3103, 16444, 10064, 18169,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0],\n",
       "       [26249, 34141, 18169, 16162, 33417, 13943,  6284, 18169,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = Preprocessor(config)\n",
    "preprocessor.transform(['性价比不错，交通方便。', '房间太小。其他的都一般。'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9f22f4e-e0c2-462d-b36b-b612d5f722c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "class TextCNN(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.preprocessor = Preprocessor(config)\n",
    "        self.class_name = {0: '负面', 1: '正面'}\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)  # 降低学习率\n",
    "\n",
    "    def build_model(self):\n",
    "        # 输入层\n",
    "        idx_input = tf.keras.layers.Input((self.config.max_seq_len,))\n",
    "        \n",
    "        # 嵌入层（增加trainable参数控制）\n",
    "        input_embedding = tf.keras.layers.Embedding(\n",
    "            len(self.preprocessor.token2idx),\n",
    "            self.config.embedding_dim,\n",
    "            input_length=self.config.max_seq_len,\n",
    "            mask_zero=True,\n",
    "            embeddings_regularizer=regularizers.l2(1e-4)  # 添加嵌入层正则化\n",
    "        )(idx_input)\n",
    "\n",
    "        # 多尺度卷积层（减少滤波器数量）\n",
    "        conv_blocks = []\n",
    "        for kernel_size in [3, 4, 5]:\n",
    "            conv = tf.keras.layers.Conv1D(\n",
    "                filters=64,  # 减少滤波器数量\n",
    "                kernel_size=kernel_size,\n",
    "                activation='relu',\n",
    "                padding='valid',\n",
    "                kernel_regularizer=regularizers.l2(1e-4)  # 卷积核正则化\n",
    "            )(input_embedding)\n",
    "            pool = tf.keras.layers.GlobalMaxPooling1D()(conv)\n",
    "            conv_blocks.append(pool)\n",
    "\n",
    "        # 特征拼接\n",
    "        concat = tf.keras.layers.concatenate(conv_blocks, axis=-1)\n",
    "        \n",
    "        # 添加批标准化层\n",
    "        bn = tf.keras.layers.BatchNormalization()(concat)\n",
    "        \n",
    "        # 全连接层（减少神经元数量）\n",
    "        dense = tf.keras.layers.Dense(\n",
    "            64,  # 减少神经元数量\n",
    "            activation='relu',\n",
    "            kernel_regularizer=regularizers.l2(0.01)  # 增强L2正则化\n",
    "        )(bn)\n",
    "        \n",
    "        # 增强Dropout\n",
    "        dropout = tf.keras.layers.Dropout(0.7)(dense)  # 提高dropout比例\n",
    "        \n",
    "        # 输出层\n",
    "        output = tf.keras.layers.Dense(2, activation='softmax')(dropout)\n",
    "\n",
    "        # 编译模型\n",
    "        model = tf.keras.Model(inputs=idx_input, outputs=output)\n",
    "        model.compile(\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            optimizer=self.optimizer,  # 使用自定义优化器\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        model.summary()\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, x_train, y_train, x_valid=None, y_valid=None, epochs=50, batch_size=64, **kwargs):\n",
    "    # 初始化回调\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_accuracy',  # 改为监控准确率\n",
    "            patience=10,  # 增大耐心值\n",
    "            mode='max',\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    \n",
    "        lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "            './improved_cnn_checkpoint',\n",
    "            save_weights_only=True,\n",
    "            save_best_only=True,\n",
    "            monitor='val_accuracy'\n",
    "        )\n",
    "\n",
    "        # 数据预处理\n",
    "        self.build_model()\n",
    "        x_train = self.preprocessor.transform(x_train)\n",
    "        valid_data = (self.preprocessor.transform(x_valid), y_valid) if x_valid is not None else None\n",
    "\n",
    "        # 合并显式传递的回调和 kwargs 中的回调\n",
    "        callbacks = [early_stop, lr_scheduler, checkpoint] + kwargs.get('callbacks', [])\n",
    "\n",
    "        # 过滤掉 kwargs 中的 callbacks 参数\n",
    "        filtered_kwargs = {k: v for k, v in kwargs.items() if k != 'callbacks'}\n",
    "\n",
    "        # 开始训练\n",
    "        history = self.model.fit(\n",
    "            x=x_train,\n",
    "            y=y_train,\n",
    "            validation_data=valid_data,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,  # 显式传递合并后的回调\n",
    "            **filtered_kwargs  # 过滤掉 callbacks 后的其他参数\n",
    "        )\n",
    "        return history\n",
    "    # 保持其他方法不变\n",
    "    def evaluate(self, x_test, y_test):\n",
    "        x_test = self.preprocessor.transform(x_test)\n",
    "        y_pred = np.argmax(self.model.predict(x_test), axis=-1)\n",
    "        print(classification_report(y_test, y_pred, target_names=['负面', '正面']))\n",
    "    \n",
    "    def single_predict(self, text):\n",
    "        x = self.preprocessor.transform([text])\n",
    "        prob = self.model.predict(x)[0]\n",
    "        label_idx = np.argmax(prob)\n",
    "        return self.class_name[label_idx], prob[label_idx]\n",
    "    \n",
    "    def load_model(self, ckpt_file):\n",
    "        self.build_model()\n",
    "        self.model.load_weights(ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61649fcf-1b6b-4f04-8dbf-fa814cc8d028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 10:44:23.268357: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/lib/x86_64-linux-gnu/\n",
      "2025-04-28 10:44:23.268394: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-04-28 10:44:23.268413: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (notebook-936bea3e-c2f3-4af8-a777-65a410cbc27a): /proc/driver/nvidia/version does not exist\n",
      "2025-04-28 10:44:23.268591: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "2025-04-28 10:44:23.277909: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2799995000 Hz\n",
      "2025-04-28 10:44:23.281900: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55afbe4cb5c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2025-04-28 10:44:23.281930: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 200, 300)     10527900    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 198, 64)      57664       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 197, 64)      76864       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 196, 64)      96064       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 64)           0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 64)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 64)           0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 192)          0           global_max_pooling1d[0][0]       \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 192)          768         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           12352       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 64)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            130         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 10,771,742\n",
      "Trainable params: 10,771,358\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n",
      "Train on 9146 samples, validate on 1200 samples\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma-user/anaconda3/envs/TensorFlow-2.1/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/ma-user/anaconda3/envs/TensorFlow-2.1/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9146/9146 [==============================] - 99s 11ms/sample - loss: 2.4471 - accuracy: 0.5681 - val_loss: 2.2311 - val_accuracy: 0.6650\n",
      "Epoch 2/50\n",
      "9146/9146 [==============================] - 97s 11ms/sample - loss: 2.0220 - accuracy: 0.7339 - val_loss: 2.0770 - val_accuracy: 0.8142\n",
      "Epoch 3/50\n",
      "9146/9146 [==============================] - 97s 11ms/sample - loss: 1.7811 - accuracy: 0.8326 - val_loss: 1.9005 - val_accuracy: 0.8400\n",
      "Epoch 4/50\n",
      "9146/9146 [==============================] - 98s 11ms/sample - loss: 1.5977 - accuracy: 0.8883 - val_loss: 1.7082 - val_accuracy: 0.8567\n",
      "Epoch 5/50\n",
      "9146/9146 [==============================] - 97s 11ms/sample - loss: 1.4554 - accuracy: 0.9283 - val_loss: 1.5585 - val_accuracy: 0.8700\n",
      "Epoch 6/50\n",
      "9146/9146 [==============================] - 96s 10ms/sample - loss: 1.3315 - accuracy: 0.9574 - val_loss: 1.4598 - val_accuracy: 0.8750\n",
      "Epoch 7/50\n",
      "9146/9146 [==============================] - 95s 10ms/sample - loss: 1.2214 - accuracy: 0.9718 - val_loss: 1.3833 - val_accuracy: 0.8750\n",
      "Epoch 8/50\n",
      "9146/9146 [==============================] - 96s 10ms/sample - loss: 1.1353 - accuracy: 0.9788 - val_loss: 1.3157 - val_accuracy: 0.8775\n",
      "Epoch 9/50\n",
      "9146/9146 [==============================] - 97s 11ms/sample - loss: 1.0528 - accuracy: 0.9853 - val_loss: 1.2555 - val_accuracy: 0.8792\n",
      "Epoch 10/50\n",
      "9146/9146 [==============================] - 98s 11ms/sample - loss: 0.9787 - accuracy: 0.9908 - val_loss: 1.1956 - val_accuracy: 0.8808\n",
      "Epoch 11/50\n",
      "9146/9146 [==============================] - 96s 11ms/sample - loss: 0.9161 - accuracy: 0.9910 - val_loss: 1.1393 - val_accuracy: 0.8817\n",
      "Epoch 12/50\n",
      "9146/9146 [==============================] - 95s 10ms/sample - loss: 0.8523 - accuracy: 0.9944 - val_loss: 1.0838 - val_accuracy: 0.8892\n",
      "Epoch 13/50\n",
      "9146/9146 [==============================] - 95s 10ms/sample - loss: 0.7984 - accuracy: 0.9944 - val_loss: 1.0384 - val_accuracy: 0.8867\n",
      "Epoch 14/50\n",
      "9146/9146 [==============================] - 94s 10ms/sample - loss: 0.7464 - accuracy: 0.9957 - val_loss: 0.9952 - val_accuracy: 0.8875\n",
      "Epoch 15/50\n",
      "9146/9146 [==============================] - 94s 10ms/sample - loss: 0.6979 - accuracy: 0.9961 - val_loss: 0.9528 - val_accuracy: 0.8858\n",
      "Epoch 16/50\n",
      "9146/9146 [==============================] - 95s 10ms/sample - loss: 0.6536 - accuracy: 0.9955 - val_loss: 0.9198 - val_accuracy: 0.8867\n",
      "Epoch 17/50\n",
      "9146/9146 [==============================] - 95s 10ms/sample - loss: 0.6088 - accuracy: 0.9966 - val_loss: 0.8795 - val_accuracy: 0.8850\n",
      "Epoch 18/50\n",
      "9146/9146 [==============================] - 96s 11ms/sample - loss: 0.5722 - accuracy: 0.9956 - val_loss: 0.8437 - val_accuracy: 0.8900\n",
      "Epoch 19/50\n",
      "9146/9146 [==============================] - 95s 10ms/sample - loss: 0.5356 - accuracy: 0.9958 - val_loss: 0.8191 - val_accuracy: 0.8875\n",
      "Epoch 20/50\n",
      "9146/9146 [==============================] - 94s 10ms/sample - loss: 0.5015 - accuracy: 0.9957 - val_loss: 0.7893 - val_accuracy: 0.8867\n",
      "Epoch 21/50\n",
      "9146/9146 [==============================] - 95s 10ms/sample - loss: 0.4694 - accuracy: 0.9965 - val_loss: 0.7551 - val_accuracy: 0.8883\n",
      "Epoch 22/50\n",
      "9146/9146 [==============================] - 95s 10ms/sample - loss: 0.4401 - accuracy: 0.9964 - val_loss: 0.7241 - val_accuracy: 0.8917\n",
      "Epoch 23/50\n",
      "9146/9146 [==============================] - 95s 10ms/sample - loss: 0.4138 - accuracy: 0.9963 - val_loss: 0.7033 - val_accuracy: 0.8875\n",
      "Epoch 24/50\n",
      "9146/9146 [==============================] - 96s 10ms/sample - loss: 0.3878 - accuracy: 0.9960 - val_loss: 0.6830 - val_accuracy: 0.8900\n",
      "Epoch 25/50\n",
      "9146/9146 [==============================] - 95s 10ms/sample - loss: 0.3634 - accuracy: 0.9963 - val_loss: 0.6640 - val_accuracy: 0.8858\n",
      "Epoch 26/50\n",
      "9146/9146 [==============================] - 96s 10ms/sample - loss: 0.3407 - accuracy: 0.9973 - val_loss: 0.6456 - val_accuracy: 0.8850\n",
      "Epoch 27/50\n",
      "9146/9146 [==============================] - 95s 10ms/sample - loss: 0.3216 - accuracy: 0.9962 - val_loss: 0.6193 - val_accuracy: 0.8883\n",
      "Epoch 28/50\n",
      "9146/9146 [==============================] - 94s 10ms/sample - loss: 0.3020 - accuracy: 0.9970 - val_loss: 0.6139 - val_accuracy: 0.8867\n",
      "Epoch 29/50\n",
      "9146/9146 [==============================] - 95s 10ms/sample - loss: 0.2841 - accuracy: 0.9957 - val_loss: 0.5917 - val_accuracy: 0.8850\n",
      "Epoch 30/50\n",
      "9146/9146 [==============================] - 94s 10ms/sample - loss: 0.2680 - accuracy: 0.9965 - val_loss: 0.6002 - val_accuracy: 0.8808\n",
      "Epoch 31/50\n",
      "9146/9146 [==============================] - 95s 10ms/sample - loss: 0.2518 - accuracy: 0.9964 - val_loss: 0.5802 - val_accuracy: 0.8883\n",
      "Epoch 32/50\n",
      "9146/9146 [==============================] - 95s 10ms/sample - loss: 0.2376 - accuracy: 0.9968 - val_loss: 0.5565 - val_accuracy: 0.8892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f445c0e0b50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ========== 修改后的回调函数设置 ==========\n",
    "# 1. 增强早停策略\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',  # 改为监控验证准确率\n",
    "    patience=10,            # 增大耐心值\n",
    "    mode='max',             # 指标越大越好\n",
    "    restore_best_weights=True  # 自动恢复最佳权重\n",
    ")\n",
    "\n",
    "# 2. 添加学习率调度\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',     # 监控验证损失\n",
    "    factor=0.5,             # 学习率衰减系数\n",
    "    patience=3,             # 连续3次未改进则调整\n",
    "    min_lr=1e-6             # 最小学习率\n",
    ")\n",
    "\n",
    "# 3. 优化模型保存策略\n",
    "checkpoint_prefix = './checkpoints/textcnn_imdb_ckpt'\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    "    monitor='val_accuracy',  # 与早停监控指标一致\n",
    "    mode='max'               # 保存准确率最高的模型\n",
    ")\n",
    "\n",
    "# ========== 初始化模型和训练 ==========\n",
    "textcnn = TextCNN(config)\n",
    "textcnn.fit(\n",
    "    x_train, y_train,\n",
    "    x_valid=x_valid, \n",
    "    y_valid=y_valid,\n",
    "    epochs=50,\n",
    "    callbacks=[early_stop, checkpoint_callback, lr_scheduler]  # 添加学习率调度\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "178f9c59-4cea-4daa-98d0-b56186c6f008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          负面       0.89      0.92      0.90       592\n",
      "          正面       0.92      0.89      0.90       608\n",
      "\n",
      "    accuracy                           0.90      1200\n",
      "   macro avg       0.90      0.90      0.90      1200\n",
      "weighted avg       0.90      0.90      0.90      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textcnn.evaluate(x_test, y_test) # 测试集评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1adb48b-24ea-4981-95ff-b83318681d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 200, 300)     10527900    input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 198, 64)      57664       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 197, 64)      76864       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 196, 64)      96064       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 64)           0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 64)           0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 64)           0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 192)          0           global_max_pooling1d_6[0][0]     \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "                                                                 global_max_pooling1d_8[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 192)          768         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           12352       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64)           0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 2)            130         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 10,771,742\n",
      "Trainable params: 10,771,358\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "textcnn = TextCNN(config)\n",
    "textcnn.load_model(checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "139a0168-09ac-4415-a1e6-241e1016a381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('正面', 0.9999671)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textcnn.single_predict(\"外观很漂亮，出人意料地漂亮，做工非常好\") # 单句预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d4d28c4-2693-4a37-91d1-c1352ce00704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('负面', 0.9998888)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textcnn.single_predict(\"书的内容没什么好说的，主要是纸张、印刷太差，所用的纸非常粗糙比一般的盗版书还要差，裁的也不好。\") # 单句预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26613a0b-58b4-4fb9-b6ff-e69c1c39c310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM创建\n",
    "\n",
    "# 定义 LSTM 模型类\n",
    "class TextLSTM(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.preprocessor = Preprocessor(config)\n",
    "        self.class_name = {0: '负面', 1: '正面'}\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)  # 降低学习率\n",
    "\n",
    "    def build_model(self):\n",
    "        # 输入层\n",
    "        idx_input = tf.keras.layers.Input((self.config.max_seq_len,))\n",
    "\n",
    "        # 嵌入层（增加trainable参数控制）\n",
    "        input_embedding = tf.keras.layers.Embedding(\n",
    "            len(self.preprocessor.token2idx),\n",
    "            self.config.embedding_dim,\n",
    "            input_length=self.config.max_seq_len,\n",
    "            mask_zero=True,\n",
    "            embeddings_regularizer=tf.keras.regularizers.l2(1e-4)  # 添加嵌入层正则化\n",
    "        )(idx_input)\n",
    "\n",
    "        # LSTM层\n",
    "        lstm = tf.keras.layers.LSTM(\n",
    "            units=128,  # LSTM单元数\n",
    "            return_sequences=False,  # 只返回最后一个时间步的输出\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(1e-4)  # LSTM层正则化\n",
    "        )(input_embedding)\n",
    "\n",
    "        # 添加批标准化层\n",
    "        bn = tf.keras.layers.BatchNormalization()(lstm)\n",
    "\n",
    "        # 全连接层\n",
    "        dense = tf.keras.layers.Dense(\n",
    "            64,  # 神经元数量\n",
    "            activation='relu',\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(0.01)  # 增强L2正则化\n",
    "        )(bn)\n",
    "\n",
    "        # 添加Dropout\n",
    "        dropout = tf.keras.layers.Dropout(0.5)(dense)\n",
    "\n",
    "        # 输出层\n",
    "        output = tf.keras.layers.Dense(2, activation='softmax')(dropout)\n",
    "\n",
    "        # 编译模型\n",
    "        model = tf.keras.Model(inputs=idx_input, outputs=output)\n",
    "        model.compile(\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            optimizer=self.optimizer,  # 使用自定义优化器\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        model.summary()\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, x_train, y_train, x_valid=None, y_valid=None, epochs=50, batch_size=64, **kwargs):\n",
    "        # 初始化回调\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_accuracy',  # 改为监控准确率\n",
    "            patience=10,  # 增大耐心值\n",
    "            mode='max',\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "            './improved_lstm_checkpoint',\n",
    "            save_weights_only=True,\n",
    "            save_best_only=True,\n",
    "            monitor='val_accuracy'\n",
    "        )\n",
    "\n",
    "        # 数据预处理\n",
    "        self.build_model()\n",
    "        x_train = self.preprocessor.transform(x_train)\n",
    "        valid_data = (self.preprocessor.transform(x_valid), y_valid) if x_valid is not None else None\n",
    "\n",
    "        # 合并显式传递的回调和 kwargs 中的回调\n",
    "        callbacks = [early_stop, lr_scheduler, checkpoint] + kwargs.get('callbacks', [])\n",
    "\n",
    "        # 过滤掉 kwargs 中的 callbacks 参数\n",
    "        filtered_kwargs = {k: v for k, v in kwargs.items() if k != 'callbacks'}\n",
    "\n",
    "        # 开始训练\n",
    "        history = self.model.fit(\n",
    "            x=x_train,\n",
    "            y=y_train,\n",
    "            validation_data=valid_data,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,  # 显式传递合并后的回调\n",
    "            **filtered_kwargs  # 过滤掉 callbacks 后的其他参数\n",
    "        )\n",
    "        return history\n",
    "\n",
    "    def evaluate(self, x_test, y_test):\n",
    "        x_test = self.preprocessor.transform(x_test)\n",
    "        y_pred = np.argmax(self.model.predict(x_test), axis=-1)\n",
    "        print(classification_report(y_test, y_pred, target_names=['负面', '正面']))\n",
    "\n",
    "    def single_predict(self, text):\n",
    "        x = self.preprocessor.transform([text])\n",
    "        prob = self.model.predict(x)[0]\n",
    "        label_idx = np.argmax(prob)\n",
    "        return self.class_name[label_idx], prob[label_idx]\n",
    "\n",
    "    def load_model(self, ckpt_file):\n",
    "        self.build_model()\n",
    "        self.model.load_weights(ckpt_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b880889-5d6a-4996-a34a-1876f213911e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 200, 300)          10527900  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 10,756,446\n",
      "Trainable params: 10,756,190\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "Train on 9146 samples, validate on 1200 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma-user/anaconda3/envs/TensorFlow-2.1/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/ma-user/anaconda3/envs/TensorFlow-2.1/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9146/9146 [==============================] - 176s 19ms/sample - loss: 2.1950 - accuracy: 0.5704 - val_loss: 1.9680 - val_accuracy: 0.5300\n",
      "Epoch 2/50\n",
      "9146/9146 [==============================] - 172s 19ms/sample - loss: 1.5920 - accuracy: 0.8021 - val_loss: 1.6037 - val_accuracy: 0.8267\n",
      "Epoch 3/50\n",
      "9146/9146 [==============================] - 171s 19ms/sample - loss: 1.1872 - accuracy: 0.9274 - val_loss: 1.2873 - val_accuracy: 0.8725\n",
      "Epoch 4/50\n",
      "9146/9146 [==============================] - 172s 19ms/sample - loss: 0.9633 - accuracy: 0.9680 - val_loss: 1.1159 - val_accuracy: 0.8767\n",
      "Epoch 5/50\n",
      "9146/9146 [==============================] - 172s 19ms/sample - loss: 0.8263 - accuracy: 0.9805 - val_loss: 1.0645 - val_accuracy: 0.8675\n",
      "Epoch 6/50\n",
      "9146/9146 [==============================] - 173s 19ms/sample - loss: 0.7252 - accuracy: 0.9833 - val_loss: 1.0004 - val_accuracy: 0.8733\n",
      "Epoch 7/50\n",
      "9146/9146 [==============================] - 172s 19ms/sample - loss: 0.6297 - accuracy: 0.9882 - val_loss: 0.9760 - val_accuracy: 0.8683\n",
      "Epoch 8/50\n",
      "9146/9146 [==============================] - 172s 19ms/sample - loss: 0.5571 - accuracy: 0.9893 - val_loss: 0.9609 - val_accuracy: 0.8683\n",
      "Epoch 9/50\n",
      "9146/9146 [==============================] - 172s 19ms/sample - loss: 0.4917 - accuracy: 0.9913 - val_loss: 0.9074 - val_accuracy: 0.8692\n",
      "Epoch 10/50\n",
      "9146/9146 [==============================] - 172s 19ms/sample - loss: 0.4395 - accuracy: 0.9906 - val_loss: 0.8847 - val_accuracy: 0.8683\n",
      "Epoch 11/50\n",
      "9146/9146 [==============================] - 172s 19ms/sample - loss: 0.3875 - accuracy: 0.9922 - val_loss: 0.8251 - val_accuracy: 0.8792\n",
      "Epoch 12/50\n",
      "9146/9146 [==============================] - 170s 19ms/sample - loss: 0.3459 - accuracy: 0.9932 - val_loss: 0.8699 - val_accuracy: 0.8700\n",
      "Epoch 13/50\n",
      "9146/9146 [==============================] - 171s 19ms/sample - loss: 0.3123 - accuracy: 0.9940 - val_loss: 0.8174 - val_accuracy: 0.8675\n",
      "Epoch 14/50\n",
      "9146/9146 [==============================] - 172s 19ms/sample - loss: 0.2805 - accuracy: 0.9945 - val_loss: 0.7898 - val_accuracy: 0.8717\n",
      "Epoch 15/50\n",
      "9146/9146 [==============================] - 172s 19ms/sample - loss: 0.2563 - accuracy: 0.9928 - val_loss: 0.7649 - val_accuracy: 0.8800\n",
      "Epoch 16/50\n",
      "9146/9146 [==============================] - 171s 19ms/sample - loss: 0.2375 - accuracy: 0.9928 - val_loss: 0.7421 - val_accuracy: 0.8717\n",
      "Epoch 17/50\n",
      "9146/9146 [==============================] - 171s 19ms/sample - loss: 0.2199 - accuracy: 0.9925 - val_loss: 0.7442 - val_accuracy: 0.8658\n",
      "Epoch 18/50\n",
      "9146/9146 [==============================] - 171s 19ms/sample - loss: 0.2030 - accuracy: 0.9927 - val_loss: 0.7950 - val_accuracy: 0.8583\n",
      "Epoch 19/50\n",
      "9146/9146 [==============================] - 171s 19ms/sample - loss: 0.2010 - accuracy: 0.9874 - val_loss: 0.7263 - val_accuracy: 0.8658\n",
      "Epoch 20/50\n",
      "9146/9146 [==============================] - 171s 19ms/sample - loss: 0.1802 - accuracy: 0.9922 - val_loss: 0.7398 - val_accuracy: 0.8625\n",
      "Epoch 21/50\n",
      "9146/9146 [==============================] - 171s 19ms/sample - loss: 0.1653 - accuracy: 0.9944 - val_loss: 0.7076 - val_accuracy: 0.8675\n",
      "Epoch 22/50\n",
      "9146/9146 [==============================] - 171s 19ms/sample - loss: 0.1570 - accuracy: 0.9951 - val_loss: 0.8033 - val_accuracy: 0.8642\n",
      "Epoch 23/50\n",
      "9146/9146 [==============================] - 171s 19ms/sample - loss: 0.1486 - accuracy: 0.9946 - val_loss: 0.7565 - val_accuracy: 0.8683\n",
      "Epoch 24/50\n",
      "9088/9146 [============================>.] - ETA: 1s - loss: 0.1414 - accuracy: 0.9943\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "9146/9146 [==============================] - 172s 19ms/sample - loss: 0.1413 - accuracy: 0.9943 - val_loss: 0.8579 - val_accuracy: 0.8317\n",
      "Epoch 25/50\n",
      "9146/9146 [==============================] - 172s 19ms/sample - loss: 0.1351 - accuracy: 0.9950 - val_loss: 0.7502 - val_accuracy: 0.8717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f438035ced0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初始化LSTM模型并进行训练\n",
    "textlstm = TextLSTM(config)\n",
    "textlstm.fit(\n",
    "    x_train, y_train,\n",
    "    x_valid=x_valid, \n",
    "    y_valid=y_valid,\n",
    "    epochs=50,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9771bd1-128c-42d3-b220-3d484cc96088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          负面       0.89      0.88      0.88       592\n",
      "          正面       0.88      0.89      0.89       608\n",
      "\n",
      "    accuracy                           0.88      1200\n",
      "   macro avg       0.88      0.88      0.88      1200\n",
      "weighted avg       0.88      0.88      0.88      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 对测试集进行评估\n",
    "textlstm.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52c68f83-79f2-4b7d-b34a-06d7dffc97b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 200, 300)          10527900  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 10,756,446\n",
      "Trainable params: 10,756,190\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 加载最佳模型权重\n",
    "textlstm.load_model('./improved_lstm_checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16bf00d6-d75b-401c-b76c-ea853f1da538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('正面', 0.9960388)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textlstm.single_predict(\"外观很漂亮，出人意料地漂亮，做工非常好\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7533987-6a71-4e97-be88-ba98b241931c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('负面', 0.9999999)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textlstm.single_predict(\"书的内容没什么好说的，主要是纸张、印刷太差，所用的纸非常粗糙比一般的盗版书还要差，裁的也不好。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-2.1",
   "language": "python",
   "name": "tensorflow-2.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
