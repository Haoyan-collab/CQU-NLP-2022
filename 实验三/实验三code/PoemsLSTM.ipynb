{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "793cf3e9-89c2-44ad-9424-e7a7885a91d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n",
      "Requirement already satisfied: ipywidgets in /home/ma-user/anaconda3/envs/PyTorch-1.10.2/lib/python3.7/site-packages (8.1.7)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /home/ma-user/anaconda3/envs/PyTorch-1.10.2/lib/python3.7/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/ma-user/anaconda3/envs/PyTorch-1.10.2/lib/python3.7/site-packages (from ipywidgets) (5.5.0)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /home/ma-user/anaconda3/envs/PyTorch-1.10.2/lib/python3.7/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/ma-user/anaconda3/envs/PyTorch-1.10.2/lib/python3.7/site-packages (from ipywidgets) (7.34.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/ma-user/anaconda3/envs/PyTorch-1.10.2/lib/python3.7/site-packages (from ipywidgets) (0.1.4)\n",
      "Requirement already satisfied: backcall in /home/ma-user/anaconda3/envs/PyTorch-1.10.2/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in /home/ma-user/anaconda3/envs/PyTorch-1.10.2/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/ma-user/anaconda3/envs/PyTorch-1.10.2/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets) (65.5.0)\n",
      "Requirement already satisfied: matplotlib-inline in /home/ma-user/anaconda3/envs/PyTorch-1.10.2/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: pygments in /home/ma-user/anaconda3/envs/PyTorch-1.10.2/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets) (2.13.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/ma-user/anaconda3/envs/PyTorch-1.10.2/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: decorator in /home/ma-user/anaconda3/envs/PyTorch-1.10.2/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/ma-user/anaconda3/envs/PyTorch-1.10.2/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/ma-user/anaconda3/envs/PyTorch-1.10.2/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.31)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/ma-user/anaconda3/envs/PyTorch-1.10.2/lib/python3.7/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/ma-user/anaconda3/envs/PyTorch-1.10.2/lib/python3.7/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/ma-user/anaconda3/envs/PyTorch-1.10.2/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/home/ma-user/anaconda3/envs/PyTorch-1.10.2/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipywidgets\n",
    "# 第一次需要运行，下面显示already satisfied是因为我第一次下载后因为模型问题把notebook重新跑了一遍，下意识又点了pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8edc2c6b-bc64-4349-a2d0-b9df3dda47ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import time # 导入 time 模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "126ef25b-4d1d-4aa1-aa4f-e090b8cd8c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据和映射关系加载完成。\n",
      "word2ix 类型: <class 'dict'>\n",
      "idx2word 类型: <class 'dict'>\n",
      "共有 57580 首诗。\n"
     ]
    }
   ],
   "source": [
    "# ## 2. 加载数据和映射关系 (Load Data and Mappings)\n",
    "#\n",
    "# - 从 \"tang.npz\" 文件加载数据。\n",
    "# - 提取诗歌数据、词到索引的映射 (word2ix) 和索引到词的映射 (ix2word)。\n",
    "\n",
    "\n",
    "tang_file = np.load(\"tang.npz\", allow_pickle=True)\n",
    "\n",
    "data = tang_file['data']\n",
    "word2ix = tang_file['word2ix'].item()\n",
    "idx2word = tang_file['ix2word'].item()\n",
    "\n",
    "print(\"数据和映射关系加载完成。\")\n",
    "print(f\"word2ix 类型: {type(word2ix)}\")\n",
    "print(f\"idx2word 类型: {type(idx2word)}\")\n",
    "print(f\"共有 {len(data)} 首诗。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88fb0526-2f95-4582-9b8f-48cdd42d5a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "示例诗歌 (data[314]): </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><START>南楼夜已寂，暗鸟动林间。不见城郭事，沈沈唯四山。<EOP>\n",
      "</s> 对应的索引: 8292\n"
     ]
    }
   ],
   "source": [
    "# ## 3. `idx2poem` 函数定义与示例 (idx2poem Function and Example)\n",
    "#\n",
    "# - 定义将索引序列转换为诗歌文本的函数。\n",
    "# - 打印一首示例诗歌及其特殊标记的索引。\n",
    "\n",
    "# %%\n",
    "def idx2poem(idx_poem_list):\n",
    "    poem_chars = []\n",
    "    for id_val in idx_poem_list:\n",
    "        poem_chars.append(idx2word.get(id_val, '<unk>')) # 使用 .get() 以防索引不存在\n",
    "    return \"\".join(poem_chars)\n",
    "\n",
    "print(f\"示例诗歌 (data[314]): {idx2poem(data[314])}\")\n",
    "\n",
    "# 确保 word2ix 和 '</s>' 键存在\n",
    "if '</s>' in word2ix:\n",
    "    print(f\"</s> 对应的索引: {word2ix['</s>']}\")\n",
    "else:\n",
    "    print(\"警告: 词典 word2ix 中未找到 '</s>'。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "912129cc-f04d-4607-b143-d3f14e925551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理后的第一首诗 (索引): [8291 6731 4770 1787 8118 7577 7066 4817  648 7121 1542 6483 7435 7686\n",
      " 2889 1671 5862 1949 7066 2596 4785 3629 1379 2703 7435 6064 6041 4666\n",
      " 4038 4881 7066 4747 1534   70 3788 3823 7435 4907 5567  201 2834 1519\n",
      " 7066  782  782 2063 2031  846 7435 8290]\n",
      "处理后的第一首诗 (文本): <START>度门能不访，冒雪屡西东。已想人如玉，遥怜马似骢。乍迷金谷路，稍变上阳宫。还比相思意，纷纷正满空。<EOP>\n",
      "词汇表大小: 8293\n"
     ]
    }
   ],
   "source": [
    "# ## 4. 诗歌数据预处理 (Poem Preprocessing)\n",
    "#\n",
    "# - 移除每首诗歌开头的 `</s>` 标记。\n",
    "# - 打印处理后的第一首诗歌作为示例。\n",
    "# - 打印词汇表大小。\n",
    "\n",
    "# %%\n",
    "poems = []\n",
    "for poem_indices in data:\n",
    "    start_index = 0\n",
    "    if '</s>' not in word2ix: # 检查 '</s>' 是否在词典中\n",
    "        print(\"错误: '</s>' 不在 word2ix 中，无法处理诗歌数据。将跳过预处理。\")\n",
    "        poems = data # 或者采取其他错误处理方式\n",
    "        break\n",
    "    for index, ix in enumerate(poem_indices):\n",
    "        if ix == word2ix[\"</s>\"]:\n",
    "            continue\n",
    "        else:\n",
    "            start_index = index\n",
    "            break\n",
    "    poems.append(poem_indices[start_index:])\n",
    "\n",
    "if poems:\n",
    "    print(f\"处理后的第一首诗 (索引): {poems[0]}\")\n",
    "    print(f\"处理后的第一首诗 (文本): {idx2poem(poems[0])}\")\n",
    "else:\n",
    "    print(\"警告: 处理后诗歌列表为空。\")\n",
    "\n",
    "print(f\"词汇表大小: {len(idx2word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b94b8268-d346-4c7d-8c83-9538ed7fb74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "创建的 X 序列数量: 65260\n",
      "创建的 Y 序列数量: 65260\n"
     ]
    }
   ],
   "source": [
    "# ## 5. 创建输入输出序列 (Create Input-Output Sequences)\n",
    "#\n",
    "# - 将所有诗歌连接成一个长序列。\n",
    "# - 按指定的序列长度 (`seq_len`) 切割成长序列，生成模型的输入 (`X`) 和目标 (`Y`)。\n",
    "\n",
    "# %%\n",
    "seq_len = 48\n",
    "X = []\n",
    "Y = []\n",
    "poems_data = [j for i in poems for j in i]\n",
    "\n",
    "if len(poems_data) <= seq_len :\n",
    "    print(f\"错误: poems_data 的长度 ({len(poems_data)}) 不足以创建长度为 {seq_len} 的序列。\")\n",
    "    print(\"请检查数据预处理步骤或减少 seq_len。\")\n",
    "else:\n",
    "    for i in range(0, len(poems_data) - seq_len - 1, seq_len):\n",
    "        X.append(poems_data[i:i + seq_len])\n",
    "        Y.append(poems_data[i + 1:i + seq_len + 1])\n",
    "\n",
    "    print(f\"创建的 X 序列数量: {len(X)}\")\n",
    "    print(f\"创建的 Y 序列数量: {len(Y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01586719-24b2-4660-adb7-9c80963f1245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PoemDataset 类定义完成。\n"
     ]
    }
   ],
   "source": [
    "# ## 6. 定义PyTorch数据集 (Define PyTorch Dataset)\n",
    "#\n",
    "# - 创建一个自定义的 `Dataset` 类来包装我们的 `X` 和 `Y` 数据。\n",
    "\n",
    "# %%\n",
    "class PoemDataset(Dataset):\n",
    "    def __init__(self, X_data, Y_data):\n",
    "        self.X = X_data\n",
    "        self.Y = Y_data\n",
    "        self.len = len(X_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x_np = np.array(self.X[index])\n",
    "        y_np = np.array(self.Y[index])\n",
    "        return torch.from_numpy(x_np).long(), torch.from_numpy(y_np).long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "if not X or not Y:\n",
    "    print(\"错误：X 或 Y 序列为空。后续单元格可能无法正确执行。\")\n",
    "else:\n",
    "    print(\"PoemDataset 类定义完成。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "192ae944-91a1-418f-9d8e-b2b4ba834c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader 创建完成。\n",
      "数据加载器输出的批次形状 (输入): torch.Size([128, 48])\n",
      "数据加载器输出的批次形状 (目标): torch.Size([128, 48])\n"
     ]
    }
   ],
   "source": [
    "# ## 7. 创建数据加载器 (Create DataLoader)\n",
    "#\n",
    "# - 使用 `PoemDataset` 和指定的 `batch_size` 创建一个 `DataLoader`。\n",
    "# - 从 `DataLoader` 中取一个批次的数据进行形状检查。\n",
    "\n",
    "# %%\n",
    "if not X or not Y:\n",
    "    print(\"由于 X 或 Y 为空，无法创建 DataLoader。请检查前面的步骤。\")\n",
    "else:\n",
    "    data_loader = DataLoader(PoemDataset(X, Y), batch_size=128, num_workers=0, shuffle=True)\n",
    "    print(\"DataLoader 创建完成。\")\n",
    "\n",
    "    if len(data_loader) > 0:\n",
    "        a, b = next(iter(data_loader))\n",
    "        print(f\"数据加载器输出的批次形状 (输入): {a.shape}\")\n",
    "        print(f\"数据加载器输出的批次形状 (目标): {b.shape}\")\n",
    "    else:\n",
    "        print(\"错误: DataLoader 为空，无法获取批次数据。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b8762f5-2884-438b-be11-ec67befc497d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PoemNet 模型类定义完成。\n"
     ]
    }
   ],
   "source": [
    "# ## 8. 定义模型 (`PoemNet`) (Define the Model)\n",
    "#\n",
    "# - 定义包含 Embedding 层、LSTM 层和全连接层的神经网络模型。\n",
    "\n",
    "# %%\n",
    "class PoemNet(nn.Module):\n",
    "    def __init__(self, vocab_size_param, embedding_dim_param, hidden_dim_param): # 参数名加后缀以示区分\n",
    "        super(PoemNet, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size_param, embedding_dim_param)\n",
    "        self.lstm = nn.LSTM(embedding_dim_param, hidden_dim_param, num_layers=2, batch_first=True, dropout=0.5)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim_param, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(2048, 4096),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, vocab_size_param)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_seq, hidden=None):\n",
    "        embedded = self.embeddings(input_seq)\n",
    "        lstm_out, hidden = self.lstm(embedded, hidden)\n",
    "        output = self.fc(lstm_out)\n",
    "        return output, hidden\n",
    "\n",
    "print(\"PoemNet 模型类定义完成。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5da3ee18-fc3b-4738-a91f-5ad3daaff3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用的设备: cuda:0\n",
      "\n",
      "模型结构:\n",
      "PoemNet(\n",
      "  (embeddings): Embedding(8293, 200)\n",
      "  (lstm): LSTM(200, 1024, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "    (4): Dropout(p=0.2, inplace=False)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=4096, out_features=8293, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "模型、优化器和损失函数已设置。\n"
     ]
    }
   ],
   "source": [
    "# ## 9. 模型实例化与设置 (Instantiate Model and Setup)\n",
    "#\n",
    "# - 设置词汇表大小、嵌入维度和隐藏层维度等超参数。\n",
    "# - 确定运行设备 (CPU 或 GPU)。\n",
    "# - 实例化模型，并将其移动到指定设备。\n",
    "# - 定义优化器和损失函数。\n",
    "\n",
    "# %%\n",
    "vocab_size = len(idx2word.keys())\n",
    "embedding_dim = 200\n",
    "hidden_dim = 1024\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用的设备: {device}\")\n",
    "\n",
    "my_net = PoemNet(vocab_size, embedding_dim, hidden_dim).to(device)\n",
    "print(\"\\n模型结构:\")\n",
    "print(my_net)\n",
    "\n",
    "optimizer = optim.Adam(my_net.parameters(), lr=0.001)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"\\n模型、优化器和损失函数已设置。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "216c41b3-dcd8-4193-a777-1fb866d1254a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_poem_sample 函数定义完成。\n"
     ]
    }
   ],
   "source": [
    "# ## 10. 定义诗歌生成函数 (Define Poem Generation Function)\n",
    "#\n",
    "# - `generate_poem_sample` 函数用于在训练后或训练过程中生成诗歌样本。\n",
    "\n",
    "# %%\n",
    "def generate_poem_sample(model, start_words_str, max_len=100, temperature=0.7):\n",
    "    model.eval() # 切换到评估模式\n",
    "    with torch.no_grad(): # 关闭梯度计算\n",
    "        hidden = None\n",
    "        unk_idx = word2ix.get('<unk>')\n",
    "        if unk_idx is None:\n",
    "            print(\"警告: 词典中没有 '<unk>' 标记。如果起始词未知，可能会选择词典中的第一个词。\")\n",
    "            # Fallback: use the first word from word2ix if start_words_str is unknown\n",
    "            if not word2ix: # 确保 word2ix 不是空的\n",
    "                 print(\"错误: word2ix 词典为空，无法获取备用词。\")\n",
    "                 return \"错误: word2ix 词典为空。\"\n",
    "            first_word_in_dict = next(iter(word2ix.keys()))\n",
    "            unk_idx = word2ix.get(first_word_in_dict)\n",
    "\n",
    "\n",
    "        input_seq = [word2ix.get(word, unk_idx) for word in start_words_str]\n",
    "\n",
    "        if not start_words_str or all(word not in word2ix for word in start_words_str):\n",
    "            print(\"警告: 起始词为空或所有词均未在词典中找到，将使用随机词开始。\")\n",
    "            if not ix2word:\n",
    "                print(\"错误: ix2word 为空，无法选择随机起始词。\")\n",
    "                return \"错误: ix2word 为空。\"\n",
    "            random_start_idx = random.choice(list(ix2word.keys()))\n",
    "            input_seq = [random_start_idx]\n",
    "\n",
    "        input_tensor = torch.tensor([input_seq]).long().to(device) # 确保 device 在此作用域内可用\n",
    "        generated_poem_indices = list(input_seq)\n",
    "\n",
    "        end_token_indices = {word2ix.get('</s>', -1), word2ix.get('<EOP>', -1)}\n",
    "        end_token_indices.discard(-1) # 移除-1 (如果 '</s>' 或 '<EOP>' 不存在)\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            output, hidden = model(input_tensor, hidden)\n",
    "            last_word_logits = output[0, -1, :] / temperature\n",
    "            probabilities = F.softmax(last_word_logits, dim=0)\n",
    "            next_word_idx = torch.multinomial(probabilities, 1).item()\n",
    "\n",
    "            if end_token_indices and next_word_idx in end_token_indices:\n",
    "                break\n",
    "\n",
    "            generated_poem_indices.append(next_word_idx)\n",
    "            input_tensor = torch.cat((input_tensor, torch.tensor([[next_word_idx]]).long().to(device)), dim=1)\n",
    "            if input_tensor.size(1) > seq_len: # 保持输入序列长度与训练时类似\n",
    "                 input_tensor = input_tensor[:, -seq_len:]\n",
    "        return \"\".join([ix2word.get(idx, '<unk>') for idx in generated_poem_indices])\n",
    "\n",
    "print(\"generate_poem_sample 函数定义完成。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33b72bc4-e286-4de6-b5ba-1f467c66bd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练，共 30 个 epochs...\n",
      "Epoch [1/30], Step [50/510], Loss: 6.3953\n",
      "Epoch [1/30], Step [100/510], Loss: 6.1270\n",
      "Epoch [1/30], Step [150/510], Loss: 5.9700\n",
      "Epoch [1/30], Step [200/510], Loss: 5.8381\n",
      "Epoch [1/30], Step [250/510], Loss: 5.8291\n",
      "Epoch [1/30], Step [300/510], Loss: 5.7684\n",
      "Epoch [1/30], Step [350/510], Loss: 5.7419\n",
      "Epoch [1/30], Step [400/510], Loss: 5.5905\n",
      "Epoch [1/30], Step [450/510], Loss: 5.4775\n",
      "Epoch [1/30], Step [500/510], Loss: 5.5585\n",
      "Epoch [1/30] 完成, 平均损失: 5.9702, 本Epoch用时: 290.62 秒\n",
      "--------------------------------------------------\n",
      "Epoch [2/30], Step [50/510], Loss: 5.3877\n",
      "Epoch [2/30], Step [100/510], Loss: 5.3031\n",
      "Epoch [2/30], Step [150/510], Loss: 5.3470\n",
      "Epoch [2/30], Step [200/510], Loss: 5.3079\n",
      "Epoch [2/30], Step [250/510], Loss: 5.2332\n",
      "Epoch [2/30], Step [300/510], Loss: 5.1297\n",
      "Epoch [2/30], Step [350/510], Loss: 5.2125\n",
      "Epoch [2/30], Step [400/510], Loss: 5.0158\n",
      "Epoch [2/30], Step [450/510], Loss: 5.0861\n",
      "Epoch [2/30], Step [500/510], Loss: 5.2139\n",
      "Epoch [2/30] 完成, 平均损失: 5.2430, 本Epoch用时: 294.35 秒\n",
      "--------------------------------------------------\n",
      "Epoch [3/30], Step [50/510], Loss: 5.0263\n",
      "Epoch [3/30], Step [100/510], Loss: 4.9346\n",
      "Epoch [3/30], Step [150/510], Loss: 4.9616\n",
      "Epoch [3/30], Step [200/510], Loss: 4.9195\n",
      "Epoch [3/30], Step [250/510], Loss: 4.9222\n",
      "Epoch [3/30], Step [300/510], Loss: 4.9065\n",
      "Epoch [3/30], Step [350/510], Loss: 4.8415\n",
      "Epoch [3/30], Step [400/510], Loss: 4.9131\n",
      "Epoch [3/30], Step [450/510], Loss: 4.8057\n",
      "Epoch [3/30], Step [500/510], Loss: 4.8373\n",
      "Epoch [3/30] 完成, 平均损失: 4.9137, 本Epoch用时: 295.29 秒\n",
      "--------------------------------------------------\n",
      "Epoch [4/30], Step [50/510], Loss: 4.7554\n",
      "Epoch [4/30], Step [100/510], Loss: 4.7220\n",
      "Epoch [4/30], Step [150/510], Loss: 4.6483\n",
      "Epoch [4/30], Step [200/510], Loss: 4.7792\n",
      "Epoch [4/30], Step [250/510], Loss: 4.6914\n",
      "Epoch [4/30], Step [300/510], Loss: 4.7347\n",
      "Epoch [4/30], Step [350/510], Loss: 4.6373\n",
      "Epoch [4/30], Step [400/510], Loss: 4.6419\n",
      "Epoch [4/30], Step [450/510], Loss: 4.7188\n",
      "Epoch [4/30], Step [500/510], Loss: 4.6658\n",
      "Epoch [4/30] 完成, 平均损失: 4.6866, 本Epoch用时: 295.47 秒\n",
      "--------------------------------------------------\n",
      "Epoch [5/30], Step [50/510], Loss: 4.4948\n",
      "Epoch [5/30], Step [100/510], Loss: 4.4151\n",
      "Epoch [5/30], Step [150/510], Loss: 4.4716\n",
      "Epoch [5/30], Step [200/510], Loss: 4.5073\n",
      "Epoch [5/30], Step [250/510], Loss: 4.5599\n",
      "Epoch [5/30], Step [300/510], Loss: 4.4989\n",
      "Epoch [5/30], Step [350/510], Loss: 4.5373\n",
      "Epoch [5/30], Step [400/510], Loss: 4.5142\n",
      "Epoch [5/30], Step [450/510], Loss: 4.5130\n",
      "Epoch [5/30], Step [500/510], Loss: 4.5414\n",
      "Epoch [5/30] 完成, 平均损失: 4.5137, 本Epoch用时: 293.78 秒\n",
      "--------------------------------------------------\n",
      "Epoch [6/30], Step [50/510], Loss: 4.3389\n",
      "Epoch [6/30], Step [100/510], Loss: 4.4501\n",
      "Epoch [6/30], Step [150/510], Loss: 4.3703\n",
      "Epoch [6/30], Step [200/510], Loss: 4.4374\n",
      "Epoch [6/30], Step [250/510], Loss: 4.3931\n",
      "Epoch [6/30], Step [300/510], Loss: 4.4302\n",
      "Epoch [6/30], Step [350/510], Loss: 4.3917\n",
      "Epoch [6/30], Step [400/510], Loss: 4.3383\n",
      "Epoch [6/30], Step [450/510], Loss: 4.4384\n",
      "Epoch [6/30], Step [500/510], Loss: 4.3998\n",
      "Epoch [6/30] 完成, 平均损失: 4.3736, 本Epoch用时: 292.40 秒\n",
      "--------------------------------------------------\n",
      "Epoch [7/30], Step [50/510], Loss: 4.2720\n",
      "Epoch [7/30], Step [100/510], Loss: 4.2119\n",
      "Epoch [7/30], Step [150/510], Loss: 4.2454\n",
      "Epoch [7/30], Step [200/510], Loss: 4.2835\n",
      "Epoch [7/30], Step [250/510], Loss: 4.2295\n",
      "Epoch [7/30], Step [300/510], Loss: 4.3482\n",
      "Epoch [7/30], Step [350/510], Loss: 4.2768\n",
      "Epoch [7/30], Step [400/510], Loss: 4.3310\n",
      "Epoch [7/30], Step [450/510], Loss: 4.3053\n",
      "Epoch [7/30], Step [500/510], Loss: 4.3125\n",
      "Epoch [7/30] 完成, 平均损失: 4.2516, 本Epoch用时: 294.34 秒\n",
      "--------------------------------------------------\n",
      "Epoch [8/30], Step [50/510], Loss: 4.1068\n",
      "Epoch [8/30], Step [100/510], Loss: 4.0807\n",
      "Epoch [8/30], Step [150/510], Loss: 4.1469\n",
      "Epoch [8/30], Step [200/510], Loss: 4.2058\n",
      "Epoch [8/30], Step [250/510], Loss: 4.0507\n",
      "Epoch [8/30], Step [300/510], Loss: 4.1292\n",
      "Epoch [8/30], Step [350/510], Loss: 4.2019\n",
      "Epoch [8/30], Step [400/510], Loss: 4.2179\n",
      "Epoch [8/30], Step [450/510], Loss: 4.1922\n",
      "Epoch [8/30], Step [500/510], Loss: 4.2065\n",
      "Epoch [8/30] 完成, 平均损失: 4.1441, 本Epoch用时: 293.25 秒\n",
      "--------------------------------------------------\n",
      "Epoch [9/30], Step [50/510], Loss: 4.0254\n",
      "Epoch [9/30], Step [100/510], Loss: 3.9931\n",
      "Epoch [9/30], Step [150/510], Loss: 3.9968\n",
      "Epoch [9/30], Step [200/510], Loss: 4.0617\n",
      "Epoch [9/30], Step [250/510], Loss: 4.0000\n",
      "Epoch [9/30], Step [300/510], Loss: 4.1343\n",
      "Epoch [9/30], Step [350/510], Loss: 4.0200\n",
      "Epoch [9/30], Step [400/510], Loss: 4.0751\n",
      "Epoch [9/30], Step [450/510], Loss: 4.1108\n",
      "Epoch [9/30], Step [500/510], Loss: 4.1324\n",
      "Epoch [9/30] 完成, 平均损失: 4.0457, 本Epoch用时: 293.45 秒\n",
      "--------------------------------------------------\n",
      "Epoch [10/30], Step [50/510], Loss: 3.8026\n",
      "Epoch [10/30], Step [100/510], Loss: 3.8681\n",
      "Epoch [10/30], Step [150/510], Loss: 3.9395\n",
      "Epoch [10/30], Step [200/510], Loss: 3.9284\n",
      "Epoch [10/30], Step [250/510], Loss: 3.9420\n",
      "Epoch [10/30], Step [300/510], Loss: 3.9997\n",
      "Epoch [10/30], Step [350/510], Loss: 3.9670\n",
      "Epoch [10/30], Step [400/510], Loss: 3.9646\n",
      "Epoch [10/30], Step [450/510], Loss: 3.9553\n",
      "Epoch [10/30], Step [500/510], Loss: 3.9683\n",
      "Epoch [10/30] 完成, 平均损失: 3.9562, 本Epoch用时: 292.72 秒\n",
      "--------------------------------------------------\n",
      "Epoch [11/30], Step [50/510], Loss: 3.7493\n",
      "Epoch [11/30], Step [100/510], Loss: 3.8551\n",
      "Epoch [11/30], Step [150/510], Loss: 3.8193\n",
      "Epoch [11/30], Step [200/510], Loss: 3.8021\n",
      "Epoch [11/30], Step [250/510], Loss: 3.9374\n",
      "Epoch [11/30], Step [300/510], Loss: 3.8620\n",
      "Epoch [11/30], Step [350/510], Loss: 3.9733\n",
      "Epoch [11/30], Step [400/510], Loss: 3.8513\n",
      "Epoch [11/30], Step [450/510], Loss: 3.8881\n",
      "Epoch [11/30], Step [500/510], Loss: 3.8972\n",
      "Epoch [11/30] 完成, 平均损失: 3.8736, 本Epoch用时: 292.42 秒\n",
      "--------------------------------------------------\n",
      "Epoch [12/30], Step [50/510], Loss: 3.7946\n",
      "Epoch [12/30], Step [100/510], Loss: 3.6674\n",
      "Epoch [12/30], Step [150/510], Loss: 3.7962\n",
      "Epoch [12/30], Step [200/510], Loss: 3.8681\n",
      "Epoch [12/30], Step [250/510], Loss: 3.7536\n",
      "Epoch [12/30], Step [300/510], Loss: 3.8175\n",
      "Epoch [12/30], Step [350/510], Loss: 3.7903\n",
      "Epoch [12/30], Step [400/510], Loss: 3.8826\n",
      "Epoch [12/30], Step [450/510], Loss: 3.8153\n",
      "Epoch [12/30], Step [500/510], Loss: 3.8552\n",
      "Epoch [12/30] 完成, 平均损失: 3.7984, 本Epoch用时: 291.95 秒\n",
      "--------------------------------------------------\n",
      "Epoch [13/30], Step [50/510], Loss: 3.5736\n",
      "Epoch [13/30], Step [100/510], Loss: 3.6542\n",
      "Epoch [13/30], Step [150/510], Loss: 3.6111\n",
      "Epoch [13/30], Step [200/510], Loss: 3.8070\n",
      "Epoch [13/30], Step [250/510], Loss: 3.7705\n",
      "Epoch [13/30], Step [300/510], Loss: 3.7693\n",
      "Epoch [13/30], Step [350/510], Loss: 3.8003\n",
      "Epoch [13/30], Step [400/510], Loss: 3.7830\n",
      "Epoch [13/30], Step [450/510], Loss: 3.7807\n",
      "Epoch [13/30], Step [500/510], Loss: 3.8091\n",
      "Epoch [13/30] 完成, 平均损失: 3.7284, 本Epoch用时: 290.60 秒\n",
      "--------------------------------------------------\n",
      "Epoch [14/30], Step [50/510], Loss: 3.5535\n",
      "Epoch [14/30], Step [100/510], Loss: 3.5416\n",
      "Epoch [14/30], Step [150/510], Loss: 3.7127\n",
      "Epoch [14/30], Step [200/510], Loss: 3.5742\n",
      "Epoch [14/30], Step [250/510], Loss: 3.6703\n",
      "Epoch [14/30], Step [300/510], Loss: 3.6373\n",
      "Epoch [14/30], Step [350/510], Loss: 3.7459\n",
      "Epoch [14/30], Step [400/510], Loss: 3.6495\n",
      "Epoch [14/30], Step [450/510], Loss: 3.7614\n",
      "Epoch [14/30], Step [500/510], Loss: 3.7135\n",
      "Epoch [14/30] 完成, 平均损失: 3.6638, 本Epoch用时: 291.03 秒\n",
      "--------------------------------------------------\n",
      "Epoch [15/30], Step [50/510], Loss: 3.5127\n",
      "Epoch [15/30], Step [100/510], Loss: 3.5716\n",
      "Epoch [15/30], Step [150/510], Loss: 3.6145\n",
      "Epoch [15/30], Step [200/510], Loss: 3.5601\n",
      "Epoch [15/30], Step [250/510], Loss: 3.6483\n",
      "Epoch [15/30], Step [300/510], Loss: 3.5722\n",
      "Epoch [15/30], Step [350/510], Loss: 3.6592\n",
      "Epoch [15/30], Step [400/510], Loss: 3.6538\n",
      "Epoch [15/30], Step [450/510], Loss: 3.6887\n",
      "Epoch [15/30], Step [500/510], Loss: 3.6571\n",
      "Epoch [15/30] 完成, 平均损失: 3.6054, 本Epoch用时: 292.06 秒\n",
      "--------------------------------------------------\n",
      "Epoch [16/30], Step [50/510], Loss: 3.4647\n",
      "Epoch [16/30], Step [100/510], Loss: 3.4960\n",
      "Epoch [16/30], Step [150/510], Loss: 3.4954\n",
      "Epoch [16/30], Step [200/510], Loss: 3.5890\n",
      "Epoch [16/30], Step [250/510], Loss: 3.5659\n",
      "Epoch [16/30], Step [300/510], Loss: 3.6238\n",
      "Epoch [16/30], Step [350/510], Loss: 3.5831\n",
      "Epoch [16/30], Step [400/510], Loss: 3.6506\n",
      "Epoch [16/30], Step [450/510], Loss: 3.6428\n",
      "Epoch [16/30], Step [500/510], Loss: 3.6253\n",
      "Epoch [16/30] 完成, 平均损失: 3.5506, 本Epoch用时: 293.20 秒\n",
      "--------------------------------------------------\n",
      "Epoch [17/30], Step [50/510], Loss: 3.4155\n",
      "Epoch [17/30], Step [100/510], Loss: 3.4072\n",
      "Epoch [17/30], Step [150/510], Loss: 3.5009\n",
      "Epoch [17/30], Step [200/510], Loss: 3.4855\n",
      "Epoch [17/30], Step [250/510], Loss: 3.5216\n",
      "Epoch [17/30], Step [300/510], Loss: 3.5483\n",
      "Epoch [17/30], Step [350/510], Loss: 3.5438\n",
      "Epoch [17/30], Step [400/510], Loss: 3.5331\n",
      "Epoch [17/30], Step [450/510], Loss: 3.5957\n",
      "Epoch [17/30], Step [500/510], Loss: 3.5412\n",
      "Epoch [17/30] 完成, 平均损失: 3.5000, 本Epoch用时: 292.45 秒\n",
      "--------------------------------------------------\n",
      "Epoch [18/30], Step [50/510], Loss: 3.3957\n",
      "Epoch [18/30], Step [100/510], Loss: 3.3996\n",
      "Epoch [18/30], Step [150/510], Loss: 3.4513\n",
      "Epoch [18/30], Step [200/510], Loss: 3.5449\n",
      "Epoch [18/30], Step [250/510], Loss: 3.5089\n",
      "Epoch [18/30], Step [300/510], Loss: 3.5025\n",
      "Epoch [18/30], Step [350/510], Loss: 3.3958\n",
      "Epoch [18/30], Step [400/510], Loss: 3.4695\n",
      "Epoch [18/30], Step [450/510], Loss: 3.5146\n",
      "Epoch [18/30], Step [500/510], Loss: 3.5699\n",
      "Epoch [18/30] 完成, 平均损失: 3.4525, 本Epoch用时: 292.05 秒\n",
      "--------------------------------------------------\n",
      "Epoch [19/30], Step [50/510], Loss: 3.2603\n",
      "Epoch [19/30], Step [100/510], Loss: 3.3434\n",
      "Epoch [19/30], Step [150/510], Loss: 3.4106\n",
      "Epoch [19/30], Step [200/510], Loss: 3.4377\n",
      "Epoch [19/30], Step [250/510], Loss: 3.4204\n",
      "Epoch [19/30], Step [300/510], Loss: 3.3896\n",
      "Epoch [19/30], Step [350/510], Loss: 3.4346\n",
      "Epoch [19/30], Step [400/510], Loss: 3.5023\n",
      "Epoch [19/30], Step [450/510], Loss: 3.4306\n",
      "Epoch [19/30], Step [500/510], Loss: 3.4873\n",
      "Epoch [19/30] 完成, 平均损失: 3.4098, 本Epoch用时: 290.86 秒\n",
      "--------------------------------------------------\n",
      "Epoch [20/30], Step [50/510], Loss: 3.3338\n",
      "Epoch [20/30], Step [100/510], Loss: 3.2766\n",
      "Epoch [20/30], Step [150/510], Loss: 3.3393\n",
      "Epoch [20/30], Step [200/510], Loss: 3.4002\n",
      "Epoch [20/30], Step [250/510], Loss: 3.3282\n",
      "Epoch [20/30], Step [300/510], Loss: 3.3596\n",
      "Epoch [20/30], Step [350/510], Loss: 3.4554\n",
      "Epoch [20/30], Step [400/510], Loss: 3.4010\n",
      "Epoch [20/30], Step [450/510], Loss: 3.3726\n",
      "Epoch [20/30], Step [500/510], Loss: 3.4151\n",
      "Epoch [20/30] 完成, 平均损失: 3.3685, 本Epoch用时: 290.86 秒\n",
      "--------------------------------------------------\n",
      "Epoch [21/30], Step [50/510], Loss: 3.2645\n",
      "Epoch [21/30], Step [100/510], Loss: 3.2994\n",
      "Epoch [21/30], Step [150/510], Loss: 3.3147\n",
      "Epoch [21/30], Step [200/510], Loss: 3.2497\n",
      "Epoch [21/30], Step [250/510], Loss: 3.2720\n",
      "Epoch [21/30], Step [300/510], Loss: 3.3966\n",
      "Epoch [21/30], Step [350/510], Loss: 3.4166\n",
      "Epoch [21/30], Step [400/510], Loss: 3.4008\n",
      "Epoch [21/30], Step [450/510], Loss: 3.4836\n",
      "Epoch [21/30], Step [500/510], Loss: 3.4006\n",
      "Epoch [21/30] 完成, 平均损失: 3.3308, 本Epoch用时: 290.05 秒\n",
      "--------------------------------------------------\n",
      "Epoch [22/30], Step [50/510], Loss: 3.2076\n",
      "Epoch [22/30], Step [100/510], Loss: 3.1652\n",
      "Epoch [22/30], Step [150/510], Loss: 3.2472\n",
      "Epoch [22/30], Step [200/510], Loss: 3.2548\n",
      "Epoch [22/30], Step [250/510], Loss: 3.3552\n",
      "Epoch [22/30], Step [300/510], Loss: 3.2639\n",
      "Epoch [22/30], Step [350/510], Loss: 3.3519\n",
      "Epoch [22/30], Step [400/510], Loss: 3.3344\n",
      "Epoch [22/30], Step [450/510], Loss: 3.3986\n",
      "Epoch [22/30], Step [500/510], Loss: 3.4117\n",
      "Epoch [22/30] 完成, 平均损失: 3.2933, 本Epoch用时: 289.82 秒\n",
      "--------------------------------------------------\n",
      "Epoch [23/30], Step [50/510], Loss: 3.1461\n",
      "Epoch [23/30], Step [100/510], Loss: 3.1858\n",
      "Epoch [23/30], Step [150/510], Loss: 3.2174\n",
      "Epoch [23/30], Step [200/510], Loss: 3.3341\n",
      "Epoch [23/30], Step [250/510], Loss: 3.1992\n",
      "Epoch [23/30], Step [300/510], Loss: 3.2779\n",
      "Epoch [23/30], Step [350/510], Loss: 3.3279\n",
      "Epoch [23/30], Step [400/510], Loss: 3.3534\n",
      "Epoch [23/30], Step [450/510], Loss: 3.3761\n",
      "Epoch [23/30], Step [500/510], Loss: 3.3277\n",
      "Epoch [23/30] 完成, 平均损失: 3.2576, 本Epoch用时: 289.34 秒\n",
      "--------------------------------------------------\n",
      "Epoch [24/30], Step [50/510], Loss: 3.1274\n",
      "Epoch [24/30], Step [100/510], Loss: 3.1864\n",
      "Epoch [24/30], Step [150/510], Loss: 3.2171\n",
      "Epoch [24/30], Step [200/510], Loss: 3.1704\n",
      "Epoch [24/30], Step [250/510], Loss: 3.1968\n",
      "Epoch [24/30], Step [300/510], Loss: 3.2523\n",
      "Epoch [24/30], Step [350/510], Loss: 3.2649\n",
      "Epoch [24/30], Step [400/510], Loss: 3.2503\n",
      "Epoch [24/30], Step [450/510], Loss: 3.2715\n",
      "Epoch [24/30], Step [500/510], Loss: 3.3131\n",
      "Epoch [24/30] 完成, 平均损失: 3.2252, 本Epoch用时: 289.10 秒\n",
      "--------------------------------------------------\n",
      "Epoch [25/30], Step [50/510], Loss: 3.1201\n",
      "Epoch [25/30], Step [100/510], Loss: 3.1212\n",
      "Epoch [25/30], Step [150/510], Loss: 3.1065\n",
      "Epoch [25/30], Step [200/510], Loss: 3.2303\n",
      "Epoch [25/30], Step [250/510], Loss: 3.1755\n",
      "Epoch [25/30], Step [300/510], Loss: 3.1661\n",
      "Epoch [25/30], Step [350/510], Loss: 3.3300\n",
      "Epoch [25/30], Step [400/510], Loss: 3.2455\n",
      "Epoch [25/30], Step [450/510], Loss: 3.2989\n",
      "Epoch [25/30], Step [500/510], Loss: 3.2333\n",
      "Epoch [25/30] 完成, 平均损失: 3.1956, 本Epoch用时: 289.61 秒\n",
      "--------------------------------------------------\n",
      "Epoch [26/30], Step [50/510], Loss: 3.0624\n",
      "Epoch [26/30], Step [100/510], Loss: 3.0407\n",
      "Epoch [26/30], Step [150/510], Loss: 3.1306\n",
      "Epoch [26/30], Step [200/510], Loss: 3.1394\n",
      "Epoch [26/30], Step [250/510], Loss: 3.1875\n",
      "Epoch [26/30], Step [300/510], Loss: 3.2671\n",
      "Epoch [26/30], Step [350/510], Loss: 3.1803\n",
      "Epoch [26/30], Step [400/510], Loss: 3.2480\n",
      "Epoch [26/30], Step [450/510], Loss: 3.2035\n",
      "Epoch [26/30], Step [500/510], Loss: 3.2874\n",
      "Epoch [26/30] 完成, 平均损失: 3.1644, 本Epoch用时: 290.32 秒\n",
      "--------------------------------------------------\n",
      "Epoch [27/30], Step [50/510], Loss: 2.9811\n",
      "Epoch [27/30], Step [100/510], Loss: 3.0660\n",
      "Epoch [27/30], Step [150/510], Loss: 3.0764\n",
      "Epoch [27/30], Step [200/510], Loss: 3.1547\n",
      "Epoch [27/30], Step [250/510], Loss: 3.1421\n",
      "Epoch [27/30], Step [300/510], Loss: 3.1572\n",
      "Epoch [27/30], Step [350/510], Loss: 3.1834\n",
      "Epoch [27/30], Step [400/510], Loss: 3.1427\n",
      "Epoch [27/30], Step [450/510], Loss: 3.2571\n",
      "Epoch [27/30], Step [500/510], Loss: 3.2055\n",
      "Epoch [27/30] 完成, 平均损失: 3.1357, 本Epoch用时: 289.70 秒\n",
      "--------------------------------------------------\n",
      "Epoch [28/30], Step [50/510], Loss: 3.0219\n",
      "Epoch [28/30], Step [100/510], Loss: 3.0369\n",
      "Epoch [28/30], Step [150/510], Loss: 3.0564\n",
      "Epoch [28/30], Step [200/510], Loss: 3.0606\n",
      "Epoch [28/30], Step [250/510], Loss: 3.1071\n",
      "Epoch [28/30], Step [300/510], Loss: 3.1175\n",
      "Epoch [28/30], Step [350/510], Loss: 3.2502\n",
      "Epoch [28/30], Step [400/510], Loss: 3.1586\n",
      "Epoch [28/30], Step [450/510], Loss: 3.2353\n",
      "Epoch [28/30], Step [500/510], Loss: 3.2090\n",
      "Epoch [28/30] 完成, 平均损失: 3.1089, 本Epoch用时: 289.83 秒\n",
      "--------------------------------------------------\n",
      "Epoch [29/30], Step [50/510], Loss: 3.0427\n",
      "Epoch [29/30], Step [100/510], Loss: 2.9713\n",
      "Epoch [29/30], Step [150/510], Loss: 3.1096\n",
      "Epoch [29/30], Step [200/510], Loss: 3.0033\n",
      "Epoch [29/30], Step [250/510], Loss: 3.1262\n",
      "Epoch [29/30], Step [300/510], Loss: 3.1233\n",
      "Epoch [29/30], Step [350/510], Loss: 3.1645\n",
      "Epoch [29/30], Step [400/510], Loss: 3.0927\n",
      "Epoch [29/30], Step [450/510], Loss: 3.1952\n",
      "Epoch [29/30], Step [500/510], Loss: 3.2688\n",
      "Epoch [29/30] 完成, 平均损失: 3.0824, 本Epoch用时: 289.51 秒\n",
      "--------------------------------------------------\n",
      "Epoch [30/30], Step [50/510], Loss: 2.9461\n",
      "Epoch [30/30], Step [100/510], Loss: 3.0325\n",
      "Epoch [30/30], Step [150/510], Loss: 3.0595\n",
      "Epoch [30/30], Step [200/510], Loss: 2.9880\n",
      "Epoch [30/30], Step [250/510], Loss: 3.1371\n",
      "Epoch [30/30], Step [300/510], Loss: 3.0786\n",
      "Epoch [30/30], Step [350/510], Loss: 3.1046\n",
      "Epoch [30/30], Step [400/510], Loss: 3.1289\n",
      "Epoch [30/30], Step [450/510], Loss: 3.1567\n",
      "Epoch [30/30], Step [500/510], Loss: 3.1650\n",
      "Epoch [30/30] 完成, 平均损失: 3.0569, 本Epoch用时: 289.78 秒\n",
      "--------------------------------------------------\n",
      "训练完成!\n",
      "总训练用时: 8750.21 秒\n"
     ]
    }
   ],
   "source": [
    "# ## 11. 训练模型 (Train the Model)\n",
    "#\n",
    "# - 设置训练的总轮数 (`num_epochs`)。\n",
    "# - 迭代训练数据，计算损失，反向传播并更新模型参数。\n",
    "# - 打印每个 epoch 的平均损失和用时。\n",
    "\n",
    "# %%\n",
    "num_epochs = 30\n",
    "print(f\"开始训练，共 {num_epochs} 个 epochs...\")\n",
    "\n",
    "total_training_start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    my_net.train() # 设置为训练模式\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, (inputs, targets) in enumerate(data_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = my_net(inputs)\n",
    "        loss = loss_function(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(data_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_duration = epoch_end_time - epoch_start_time\n",
    "    avg_loss = total_loss / len(data_loader) if len(data_loader) > 0 else float('inf')\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] 完成, 平均损失: {avg_loss:.4f}, 本Epoch用时: {epoch_duration:.2f} 秒')\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "total_training_end_time = time.time()\n",
    "total_training_duration = total_training_end_time - total_training_start_time\n",
    "print(\"训练完成!\")\n",
    "print(f\"总训练用时: {total_training_duration:.2f} 秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "684a1e50-ba30-48eb-a693-82bde49aac68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型已保存到model.h5\n"
     ]
    }
   ],
   "source": [
    "# ## 12. 保存模型 (Save the Model)\n",
    "#\n",
    "# - 将训练好的模型的状态字典保存到文件。\n",
    "\n",
    "# %%\n",
    "torch.save(my_net,\"model.h5\")\n",
    "print(\"模型已保存到model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03084d1b-de25-4c8f-868b-f381cde5a34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型已保存到 poem_model_final_jupyter.pth\n"
     ]
    }
   ],
   "source": [
    "# 由于整个模型太大，无法从华为云下载，尝试只保存权重\n",
    "model_save_path = \"poem_model_final_jupyter.pth\"\n",
    "torch.save(my_net.state_dict(), model_save_path)\n",
    "print(f\"模型已保存到 {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1994a52f",
   "metadata": {},
   "source": [
    "权重依旧超过100mb，无法下载，遂放弃"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-1.10.2",
   "language": "python",
   "name": "pytorch-1.10.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
